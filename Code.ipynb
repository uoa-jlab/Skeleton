{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332291e0-89e2-4522-827b-404b2614173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f48cde7-172e-4c77-a995-bed443cef989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(data, noise_factor=0.1):\n",
    "    std = torch.std(data, dim=0, keepdim=True)  # 计算原始数据的标准差\n",
    "    noise = torch.randn_like(data) * (std * noise_factor)  # 生成噪声\n",
    "    return data + noise\n",
    "def add_positional_encoding(x):\n",
    "    \"\"\"\n",
    "    x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "    # 初始化位置编码矩阵 (seq_len, d_model)\n",
    "    pe = torch.zeros(seq_len, d_model, device=x.device)\n",
    "\n",
    "    position = torch.arange(seq_len, device=x.device).unsqueeze(1)  # (seq_len, 1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, device=x.device) * (-math.log(10000.0) / d_model))  # (d_model/2)\n",
    "\n",
    "    # 处理嵌入维度为奇数的情况\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term[:(d_model//2)])  # 防止越界\n",
    "\n",
    "    # 添加维度以便广播： (1, seq_len, d_model)\n",
    "    pe = pe.unsqueeze(0)\n",
    "\n",
    "    # 加到输入上 (batch_size, seq_len, d_model)\n",
    "    return x + pe\n",
    "class PressureSkeletonDataset(Dataset):\n",
    "    def __init__(self, pressure_data, skeleton_data, decoder_input):\n",
    "        self.pressure_data = torch.FloatTensor(pressure_data)\n",
    "        self.skeleton_data = torch.FloatTensor(skeleton_data)\n",
    "        self.decoder_input = torch.FloatTensor(decoder_input)\n",
    "    def __len__(self):\n",
    "        return len(self.pressure_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pressure_data[idx], self.skeleton_data[idx],self.decoder_input[idx]    \n",
    "class EnhancedSkeletonTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_joints, num_dims=3, dropout=0.1,seq_length=3,window_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # クラス属性としてnum_jointsを保存\n",
    "        self.num_joints = num_joints\n",
    "        self.num_dims = num_dims\n",
    "        self.seq_length=seq_length\n",
    "        self.window_size=window_size\n",
    "        \n",
    "        # 入力の特徴抽出を強化\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim,d_model)\n",
    "        )\n",
    "        \n",
    "        # より深いTransformerネットワーク\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,  \n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "\n",
    "        self.decoder_feature_extractor = nn.Sequential(\n",
    "            nn.Linear(num_joints*num_dims,d_model)\n",
    "        )\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(d_model*seq_length,d_model*window_size)\n",
    "        )\n",
    "        \n",
    "        # 出力層の強化\n",
    "        self.output_decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.LayerNorm(d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_joints * num_dims)\n",
    "        )\n",
    "        # スケール係数（学習可能パラメータ）\n",
    "        self.output_scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    \n",
    "    def forward(self, x, decoder_input):\n",
    "        batch_size=x.shape[0]\n",
    "        \n",
    "        # 特徴抽出\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.unsqueeze(1)\n",
    "        decoder_input = self.decoder_feature_extractor(decoder_input)\n",
    "        #decoder_input = add_positional_encoding(decoder_input)\n",
    "\n",
    "        # Transformer処理\n",
    "        transformer_output = self.transformer_encoder(features)\n",
    "        transformer_output = self.transformer_decoder(decoder_input, transformer_output)\n",
    "        #predict = transformer_output[:,transformer_output.shape[1]-1,:]\n",
    "\n",
    "        predict=transformer_output.reshape(batch_size,-1)\n",
    "        predict_next=self.predict(predict)\n",
    "        predict_next=predict_next.reshape(batch_size,self.window_size,-1)\n",
    "        \n",
    "        # 出力生成とスケーリング\n",
    "        output = self.output_decoder(predict_next)\n",
    "        #output = self.constraint(output)\n",
    "        output = output * self.output_scale  # 出力のスケーリング\n",
    "\n",
    "        \n",
    "        return output\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, save_path, device):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for pressure, skeleton, decoder_input in train_loader:\n",
    "            # データをGPUに移動\n",
    "            pressure = pressure.to(device)\n",
    "            skeleton = skeleton.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_input = add_positional_encoding(decoder_input)\n",
    "            \n",
    "            pressure = add_gaussian_noise(pressure, noise_factor=0.1)\n",
    "            decoder_input =  add_gaussian_noise(decoder_input, noise_factor=0.1)\n",
    "            if torch.rand(1).item() < 0.95:\n",
    "                decoder_input=torch.zeros_like(decoder_input)\n",
    "            outputs = model(pressure,decoder_input)\n",
    "            loss = criterion(outputs,skeleton)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for pressure, skeleton,decoder_input in val_loader:\n",
    "                # データをGPUに移動\n",
    "                pressure = pressure.to(device)\n",
    "                skeleton = skeleton.to(device)\n",
    "                decoder_input = decoder_input.to(device)\n",
    "                \n",
    "                outputs = model(pressure,decoder_input)\n",
    "                loss = criterion(outputs, skeleton)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # 平均損失の計算\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # スケジューラのステップ\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Learning Rate: {current_lr:.6f}')\n",
    "        \n",
    "        # モデルの保存\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, save_path)\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "        \n",
    "        print('-' * 60)\n",
    "class EnhancedSkeletonLoss_WithAngleConstrains(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=0.5, window_size=5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # MSE損失\n",
    "        # 假设 pred 和 target 的原始形状为 [batch_size, window_size * num_joints * 3]\n",
    "        batch_size = int(pred.shape[0])\n",
    "        num_joints = int(pred.shape[2] // 3)\n",
    "        \n",
    "        # 重塑为 [batch_size, window_size, num_joints, 3]\n",
    "        pred_reshaped = pred.view(batch_size, self.window_size, num_joints, 3)\n",
    "        target_reshaped = target.view(batch_size, self.window_size, num_joints, 3)\n",
    "        \n",
    "        # 定义关节点权重，默认全部为 1.0\n",
    "        joint_weights = torch.tensor(0.2, device=pred.device)*torch.ones(num_joints, device=pred.device)\n",
    "        # 对背骨的关节点（索引 0～4）赋予较高权重\n",
    "        for idx in [0, 1, 2, 3, 4]:\n",
    "            joint_weights[idx] = 2.0\n",
    "        # 对两个腿的关节点（例如：一侧 13～16，另一侧 17～20）赋予较高权重\n",
    "        for idx in [13, 14, 15, 16, 17, 18, 19, 20]:\n",
    "            joint_weights[idx] = 2.0\n",
    "        \n",
    "        # 计算加权均方误差\n",
    "        # 先计算每个坐标的平方误差，形状为 [batch_size, window_size, num_joints, 3]\n",
    "        squared_diff = (pred_reshaped - target_reshaped) ** 2\n",
    "        # 对坐标求和得到每个关节的误差，形状为 [batch_size, window_size, num_joints]\n",
    "        squared_diff = squared_diff.sum(dim=-1)\n",
    "        # 将关节点的权重扩展到 [1, 1, num_joints] 后相乘\n",
    "        weighted_squared_diff = squared_diff * joint_weights.view(1, 1, num_joints)\n",
    "        # 平均得到加权的均方误差\n",
    "        mse_loss = weighted_squared_diff.mean()\n",
    "\n",
    "        \n",
    "        eps = 1e-6\n",
    "        angle_loss = 0.0\n",
    "        angle_pairs = [\n",
    "            ((0, 1), (1, 2)),\n",
    "            ((1, 2), (2, 3)),\n",
    "            ((2, 3), (3, 4)),\n",
    "            ((13, 17), (17, 18)),\n",
    "            ((13, 17), (13, 14)),\n",
    "            ((17, 18), (18, 19)),\n",
    "            ((18, 19), (19, 20)),\n",
    "            ((13, 14), (14, 15)),\n",
    "            ((14, 15), (15, 16))\n",
    "        ]\n",
    "        for (bone1, bone2) in angle_pairs:\n",
    "            # 预测向量计算\n",
    "            pred_vec1 = pred_reshaped[:, :, bone1[1], :] - pred_reshaped[:, :, bone1[0], :]\n",
    "            pred_vec2 = pred_reshaped[:, :, bone2[1], :] - pred_reshaped[:, :, bone2[0], :]\n",
    "            dot_pred = (pred_vec1 * pred_vec2).sum(dim=-1)\n",
    "            norm_pred1 = torch.norm(pred_vec1, dim=-1)\n",
    "            norm_pred2 = torch.norm(pred_vec2, dim=-1)\n",
    "            cos_pred = dot_pred / (norm_pred1 * norm_pred2 + eps)\n",
    "            cos_pred = torch.clamp(cos_pred, -1.0, 1.0)\n",
    "            \n",
    "            # 目标向量计算\n",
    "            target_vec1 = target_reshaped[:, :, bone1[1], :] - target_reshaped[:, :, bone1[0], :]\n",
    "            target_vec2 = target_reshaped[:, :, bone2[1], :] - target_reshaped[:, :, bone2[0], :]\n",
    "            dot_target = (target_vec1 * target_vec2).sum(dim=-1)\n",
    "            norm_target1 = torch.norm(target_vec1, dim=-1)\n",
    "            norm_target2 = torch.norm(target_vec2, dim=-1)\n",
    "            cos_target = dot_target / (norm_target1 * norm_target2 + eps)\n",
    "            cos_target = torch.clamp(cos_target, -1.0, 1.0)\n",
    "            \n",
    "            # 直接比较余弦值的差异\n",
    "            angle_loss += F.mse_loss(cos_pred, cos_target)\n",
    "        angle_loss = angle_loss / len(angle_pairs)\n",
    "    \n",
    "        return self.alpha * mse_loss + self.gamma*angle_loss\n",
    "\n",
    "def load_model(model, optimizer, scheduler, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "    return model, optimizer, scheduler, epoch, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241bcbf2-1b02-4bf2-a569-526eed7aae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_data(file_pairs,seq_length,window_size):\n",
    "    \"\"\"複数のデータセットを読み込んで結合する\"\"\"\n",
    "    all_skeleton_data = []\n",
    "    all_pressure_left = []\n",
    "    all_pressure_right = []\n",
    "    all_decoder_input = []\n",
    "\n",
    "    all_skeleton_label = []\n",
    "    for skeleton_file, left_file, right_file in file_pairs:\n",
    "        skeleton = pd.read_csv(skeleton_file)\n",
    "        left = pd.read_csv(left_file, dtype=float, low_memory=False)\n",
    "        right = pd.read_csv(right_file, dtype=float, low_memory=False)\n",
    "        # データ長を揃える\n",
    "        min_length = min(len(skeleton), len(left), len(right))\n",
    "\n",
    "        num_joints_points=skeleton.shape[1]\n",
    "        decoder_input = np.zeros((min_length,seq_length,num_joints_points))\n",
    "        for i in range(min_length):\n",
    "            for j in range(1,seq_length+1):\n",
    "                if i-j<0:\n",
    "                    continue\n",
    "                else:\n",
    "                    decoder_input[i,seq_length-j]=skeleton.iloc[i-j]\n",
    "\n",
    "        skeleton_label = np.zeros((min_length,window_size,num_joints_points))\n",
    "        for i in range(min_length):\n",
    "            for j in range(window_size):\n",
    "                if i+j<min_length:\n",
    "                    skeleton_label[i,j]=skeleton.iloc[i+j]\n",
    "        \n",
    "        all_skeleton_data.append(skeleton.iloc[:min_length])\n",
    "        all_pressure_left.append(left.iloc[:min_length])\n",
    "        all_pressure_right.append(right.iloc[:min_length])\n",
    "        all_decoder_input.append(decoder_input)\n",
    "        all_skeleton_label.append(skeleton_label)\n",
    "        \n",
    "    return (pd.concat(all_skeleton_data, ignore_index=True),\n",
    "            pd.concat(all_pressure_left, ignore_index=True),\n",
    "            pd.concat(all_pressure_right, ignore_index=True),\n",
    "            np.concatenate(all_decoder_input),\n",
    "            np.concatenate(all_skeleton_label))\n",
    "\n",
    "def preprocess_pressure_data(left_data, right_data):\n",
    "    \"\"\"圧力、回転、加速度データの前処理\"\"\"\n",
    "    \n",
    "    # 左足データから各種センサー値を抽出\n",
    "    left_pressure = left_data.iloc[:, :35]  # 圧力センサーの列を適切に指定\n",
    "    left_rotation = left_data.iloc[:, 35:38]  # 回転データの列を適切に指定\n",
    "    left_accel = left_data.iloc[:, 38:41]  # 加速度データの列を適切に指定\n",
    "\n",
    "    # 右足データから各種センサー値を抽出\n",
    "    right_pressure = right_data.iloc[:, :35]  # 圧力センサーの列を適切に指定\n",
    "    right_rotation = right_data.iloc[:, 35:38]  # 回転データの列を適切に指定\n",
    "    right_accel = right_data.iloc[:, 38:41]  # 加速度データの列を適切に指定\n",
    "\n",
    "    # データの結合(按列（属性）相拼接)\n",
    "    pressure_combined = pd.concat([left_pressure, right_pressure], axis=1)\n",
    "    rotation_combined = pd.concat([left_rotation, right_rotation], axis=1)\n",
    "    accel_combined = pd.concat([left_accel, right_accel], axis=1)\n",
    "\n",
    "    # NaN値を補正\n",
    "    pressure_combined = pressure_combined.fillna(0.0)\n",
    "    rotation_combined = rotation_combined.fillna(0.0)\n",
    "    accel_combined = accel_combined.fillna(0.0)\n",
    "\n",
    "    print(\"Checking pressure data for NaN or Inf...\")\n",
    "    print(\"Pressure NaN count:\", pressure_combined.isna().sum().sum())\n",
    "    print(\"Pressure Inf count:\", np.isinf(pressure_combined).sum().sum())\n",
    "\n",
    "    # 移動平均フィルタの適用\n",
    "    window_size = 3\n",
    "    pressure_combined = pressure_combined.rolling(window=window_size, center=True).mean()\n",
    "    rotation_combined = rotation_combined.rolling(window=window_size, center=True).mean()\n",
    "    accel_combined = accel_combined.rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    # NaN値を前後の値で補間\n",
    "    pressure_combined = pressure_combined.bfill().ffill()\n",
    "    rotation_combined = rotation_combined.bfill().ffill()\n",
    "    accel_combined = accel_combined.bfill().ffill()\n",
    "\n",
    "    # 正規化と標準化のスケーラー初期化\n",
    "    pressure_normalizer = MinMaxScaler()\n",
    "    rotation_normalizer = MinMaxScaler()\n",
    "    accel_normalizer = MinMaxScaler()\n",
    "\n",
    "    pressure_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    rotation_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    accel_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    # データの正規化と標準化\n",
    "    pressure_processed = pressure_standardizer.fit_transform(\n",
    "        pressure_normalizer.fit_transform(pressure_combined)\n",
    "    )\n",
    "    rotation_processed = rotation_standardizer.fit_transform(\n",
    "        rotation_normalizer.fit_transform(rotation_combined)\n",
    "    )\n",
    "    accel_processed = accel_standardizer.fit_transform(\n",
    "        accel_normalizer.fit_transform(accel_combined)\n",
    "    )\n",
    "    \n",
    "    # すべての特徴量を結合\n",
    "    input_features = np.concatenate([\n",
    "        pressure_processed,\n",
    "        rotation_processed,\n",
    "        accel_processed,\n",
    "    ], axis=1)\n",
    "\n",
    "    return input_features, {\n",
    "        'pressure': {\n",
    "            'normalizer': pressure_normalizer,\n",
    "            'standardizer': pressure_standardizer\n",
    "        },\n",
    "        'rotation': {\n",
    "            'normalizer': rotation_normalizer,\n",
    "            'standardizer': rotation_standardizer\n",
    "        },\n",
    "        'accel': {\n",
    "            'normalizer': accel_normalizer,\n",
    "            'standardizer': accel_standardizer\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc114f1-cfac-4145-8f6b-d5b525c80208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking pressure data for NaN or Inf...\n",
      "Pressure NaN count: 0\n",
      "Pressure Inf count: 0\n",
      "(62782, 82)\n",
      "(50225, 3, 63)\n",
      "(12557, 3, 63)\n",
      "Using device: cuda:0\n",
      "Checking final training and validation data...\n",
      "Train input NaN count: 0 Inf count: 0\n",
      "Train skeleton NaN count: 0 Inf count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 914465.2783\n",
      "Validation Loss: 699580.5417\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 1\n",
      "------------------------------------------------------------\n",
      "Epoch 2\n",
      "Training Loss: 450993.6691\n",
      "Validation Loss: 218231.1545\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 2\n",
      "------------------------------------------------------------\n",
      "Epoch 3\n",
      "Training Loss: 89195.3209\n",
      "Validation Loss: 37378.4151\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 3\n",
      "------------------------------------------------------------\n",
      "Epoch 4\n",
      "Training Loss: 33774.5938\n",
      "Validation Loss: 32452.1544\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 4\n",
      "------------------------------------------------------------\n",
      "Epoch 5\n",
      "Training Loss: 27401.0942\n",
      "Validation Loss: 24911.6005\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 5\n",
      "------------------------------------------------------------\n",
      "Epoch 6\n",
      "Training Loss: 25881.7049\n",
      "Validation Loss: 23988.9164\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 6\n",
      "------------------------------------------------------------\n",
      "Epoch 7\n",
      "Training Loss: 22568.7767\n",
      "Validation Loss: 20288.4357\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 7\n",
      "------------------------------------------------------------\n",
      "Epoch 8\n",
      "Training Loss: 20181.1864\n",
      "Validation Loss: 18804.3207\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 8\n",
      "------------------------------------------------------------\n",
      "Epoch 9\n",
      "Training Loss: 19055.3866\n",
      "Validation Loss: 17590.7453\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 9\n",
      "------------------------------------------------------------\n",
      "Epoch 10\n",
      "Training Loss: 18516.4694\n",
      "Validation Loss: 17185.7802\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 10\n",
      "------------------------------------------------------------\n",
      "Epoch 11\n",
      "Training Loss: 17356.3471\n",
      "Validation Loss: 15216.5379\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 11\n",
      "------------------------------------------------------------\n",
      "Epoch 12\n",
      "Training Loss: 16247.0167\n",
      "Validation Loss: 14181.5891\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 12\n",
      "------------------------------------------------------------\n",
      "Epoch 13\n",
      "Training Loss: 15241.4687\n",
      "Validation Loss: 12990.6207\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 13\n",
      "------------------------------------------------------------\n",
      "Epoch 14\n",
      "Training Loss: 14303.2913\n",
      "Validation Loss: 12640.2039\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 14\n",
      "------------------------------------------------------------\n",
      "Epoch 15\n",
      "Training Loss: 13791.4705\n",
      "Validation Loss: 12130.0891\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 15\n",
      "------------------------------------------------------------\n",
      "Epoch 16\n",
      "Training Loss: 13340.2408\n",
      "Validation Loss: 11285.5799\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 16\n",
      "------------------------------------------------------------\n",
      "Epoch 17\n",
      "Training Loss: 13053.1981\n",
      "Validation Loss: 11161.6541\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 17\n",
      "------------------------------------------------------------\n",
      "Epoch 18\n",
      "Training Loss: 12801.6626\n",
      "Validation Loss: 10965.2476\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 18\n",
      "------------------------------------------------------------\n",
      "Epoch 19\n",
      "Training Loss: 12620.0171\n",
      "Validation Loss: 10766.9186\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 19\n",
      "------------------------------------------------------------\n",
      "Epoch 20\n",
      "Training Loss: 12305.6685\n",
      "Validation Loss: 10392.8319\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 20\n",
      "------------------------------------------------------------\n",
      "Epoch 21\n",
      "Training Loss: 12009.4122\n",
      "Validation Loss: 10145.7490\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 21\n",
      "------------------------------------------------------------\n",
      "Epoch 22\n",
      "Training Loss: 11559.9619\n",
      "Validation Loss: 9693.9458\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 22\n",
      "------------------------------------------------------------\n",
      "Epoch 23\n",
      "Training Loss: 11140.1062\n",
      "Validation Loss: 9426.7065\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 23\n",
      "------------------------------------------------------------\n",
      "Epoch 24\n",
      "Training Loss: 10612.6897\n",
      "Validation Loss: 8724.4829\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 24\n",
      "------------------------------------------------------------\n",
      "Epoch 25\n",
      "Training Loss: 10310.6122\n",
      "Validation Loss: 8527.0501\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 25\n",
      "------------------------------------------------------------\n",
      "Epoch 26\n",
      "Training Loss: 10018.6232\n",
      "Validation Loss: 8169.9810\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 26\n",
      "------------------------------------------------------------\n",
      "Epoch 27\n",
      "Training Loss: 9666.4709\n",
      "Validation Loss: 7870.4840\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 27\n",
      "------------------------------------------------------------\n",
      "Epoch 28\n",
      "Training Loss: 9261.8834\n",
      "Validation Loss: 7205.1505\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 28\n",
      "------------------------------------------------------------\n",
      "Epoch 29\n",
      "Training Loss: 8615.4892\n",
      "Validation Loss: 6706.4946\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 29\n",
      "------------------------------------------------------------\n",
      "Epoch 30\n",
      "Training Loss: 8082.3347\n",
      "Validation Loss: 6260.4781\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 30\n",
      "------------------------------------------------------------\n",
      "Epoch 31\n",
      "Training Loss: 7624.4388\n",
      "Validation Loss: 5949.4877\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 31\n",
      "------------------------------------------------------------\n",
      "Epoch 32\n",
      "Training Loss: 7205.5320\n",
      "Validation Loss: 5394.5600\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 32\n",
      "------------------------------------------------------------\n",
      "Epoch 33\n",
      "Training Loss: 6769.3028\n",
      "Validation Loss: 5038.0254\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 33\n",
      "------------------------------------------------------------\n",
      "Epoch 34\n",
      "Training Loss: 6557.4032\n",
      "Validation Loss: 4757.1387\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 34\n",
      "------------------------------------------------------------\n",
      "Epoch 35\n",
      "Training Loss: 6261.5330\n",
      "Validation Loss: 4443.7806\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 35\n",
      "------------------------------------------------------------\n",
      "Epoch 36\n",
      "Training Loss: 6014.9403\n",
      "Validation Loss: 4268.0648\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 36\n",
      "------------------------------------------------------------\n",
      "Epoch 37\n",
      "Training Loss: 5807.2502\n",
      "Validation Loss: 4087.3192\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 37\n",
      "------------------------------------------------------------\n",
      "Epoch 38\n",
      "Training Loss: 5639.8518\n",
      "Validation Loss: 4009.4643\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 38\n",
      "------------------------------------------------------------\n",
      "Epoch 39\n",
      "Training Loss: 5395.5237\n",
      "Validation Loss: 3698.2143\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 39\n",
      "------------------------------------------------------------\n",
      "Epoch 40\n",
      "Training Loss: 5175.0030\n",
      "Validation Loss: 3631.3715\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 40\n",
      "------------------------------------------------------------\n",
      "Epoch 41\n",
      "Training Loss: 5055.2091\n",
      "Validation Loss: 3540.7477\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 41\n",
      "------------------------------------------------------------\n",
      "Epoch 42\n",
      "Training Loss: 4881.9040\n",
      "Validation Loss: 3331.7209\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 42\n",
      "------------------------------------------------------------\n",
      "Epoch 43\n",
      "Training Loss: 4781.8679\n",
      "Validation Loss: 3328.4901\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 43\n",
      "------------------------------------------------------------\n",
      "Epoch 44\n",
      "Training Loss: 4686.6699\n",
      "Validation Loss: 3157.2927\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 44\n",
      "------------------------------------------------------------\n",
      "Epoch 45\n",
      "Training Loss: 4584.9641\n",
      "Validation Loss: 3076.5097\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 45\n",
      "------------------------------------------------------------\n",
      "Epoch 46\n",
      "Training Loss: 4503.3615\n",
      "Validation Loss: 3041.6754\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 46\n",
      "------------------------------------------------------------\n",
      "Epoch 47\n",
      "Training Loss: 4445.2098\n",
      "Validation Loss: 3029.6573\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 47\n",
      "------------------------------------------------------------\n",
      "Epoch 48\n",
      "Training Loss: 4327.0414\n",
      "Validation Loss: 2907.4724\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 48\n",
      "------------------------------------------------------------\n",
      "Epoch 49\n",
      "Training Loss: 4313.6893\n",
      "Validation Loss: 2887.1796\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 49\n",
      "------------------------------------------------------------\n",
      "Epoch 50\n",
      "Training Loss: 4196.6329\n",
      "Validation Loss: 2793.2336\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 50\n",
      "------------------------------------------------------------\n",
      "Epoch 51\n",
      "Training Loss: 4138.9566\n",
      "Validation Loss: 2712.4862\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 51\n",
      "------------------------------------------------------------\n",
      "Epoch 52\n",
      "Training Loss: 4064.1394\n",
      "Validation Loss: 2662.2691\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 52\n",
      "------------------------------------------------------------\n",
      "Epoch 53\n",
      "Training Loss: 3944.2153\n",
      "Validation Loss: 2584.1118\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 53\n",
      "------------------------------------------------------------\n",
      "Epoch 54\n",
      "Training Loss: 3909.3768\n",
      "Validation Loss: 2499.0560\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 54\n",
      "------------------------------------------------------------\n",
      "Epoch 55\n",
      "Training Loss: 3800.5304\n",
      "Validation Loss: 2537.5945\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 56\n",
      "Training Loss: 3718.3747\n",
      "Validation Loss: 2404.8982\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 56\n",
      "------------------------------------------------------------\n",
      "Epoch 57\n",
      "Training Loss: 3610.7954\n",
      "Validation Loss: 2278.8158\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 57\n",
      "------------------------------------------------------------\n",
      "Epoch 58\n",
      "Training Loss: 3536.4608\n",
      "Validation Loss: 2205.4688\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 58\n",
      "------------------------------------------------------------\n",
      "Epoch 59\n",
      "Training Loss: 3516.9915\n",
      "Validation Loss: 2183.3455\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 59\n",
      "------------------------------------------------------------\n",
      "Epoch 60\n",
      "Training Loss: 3483.1243\n",
      "Validation Loss: 2116.4099\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 60\n",
      "------------------------------------------------------------\n",
      "Epoch 61\n",
      "Training Loss: 3373.9944\n",
      "Validation Loss: 2070.9372\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 61\n",
      "------------------------------------------------------------\n",
      "Epoch 62\n",
      "Training Loss: 3291.0062\n",
      "Validation Loss: 2072.4402\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 63\n",
      "Training Loss: 3270.1015\n",
      "Validation Loss: 2027.1178\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 63\n",
      "------------------------------------------------------------\n",
      "Epoch 64\n",
      "Training Loss: 3215.7262\n",
      "Validation Loss: 1912.5925\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 64\n",
      "------------------------------------------------------------\n",
      "Epoch 65\n",
      "Training Loss: 3205.7504\n",
      "Validation Loss: 1922.4646\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 66\n",
      "Training Loss: 3175.4342\n",
      "Validation Loss: 1899.8891\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 66\n",
      "------------------------------------------------------------\n",
      "Epoch 67\n",
      "Training Loss: 3107.3605\n",
      "Validation Loss: 2086.9827\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 68\n",
      "Training Loss: 3080.0532\n",
      "Validation Loss: 1854.9224\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 68\n",
      "------------------------------------------------------------\n",
      "Epoch 69\n",
      "Training Loss: 3056.6634\n",
      "Validation Loss: 1795.1612\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 69\n",
      "------------------------------------------------------------\n",
      "Epoch 70\n",
      "Training Loss: 3032.9388\n",
      "Validation Loss: 1870.8754\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 71\n",
      "Training Loss: 2983.2544\n",
      "Validation Loss: 1850.4778\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 72\n",
      "Training Loss: 2971.1308\n",
      "Validation Loss: 1760.8296\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 72\n",
      "------------------------------------------------------------\n",
      "Epoch 73\n",
      "Training Loss: 2889.4464\n",
      "Validation Loss: 1901.8063\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 74\n",
      "Training Loss: 2908.0923\n",
      "Validation Loss: 1716.3382\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 74\n",
      "------------------------------------------------------------\n",
      "Epoch 75\n",
      "Training Loss: 2897.8226\n",
      "Validation Loss: 1806.2240\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 76\n",
      "Training Loss: 2795.3908\n",
      "Validation Loss: 1660.4235\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 76\n",
      "------------------------------------------------------------\n",
      "Epoch 77\n",
      "Training Loss: 2827.8498\n",
      "Validation Loss: 1659.7637\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 77\n",
      "------------------------------------------------------------\n",
      "Epoch 78\n",
      "Training Loss: 2772.9394\n",
      "Validation Loss: 1627.1097\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 78\n",
      "------------------------------------------------------------\n",
      "Epoch 79\n",
      "Training Loss: 2769.9486\n",
      "Validation Loss: 1674.7811\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 80\n",
      "Training Loss: 2737.3361\n",
      "Validation Loss: 1641.9816\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 81\n",
      "Training Loss: 2728.4476\n",
      "Validation Loss: 1848.5146\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 82\n",
      "Training Loss: 2679.9199\n",
      "Validation Loss: 1552.8453\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 82\n",
      "------------------------------------------------------------\n",
      "Epoch 83\n",
      "Training Loss: 2647.2368\n",
      "Validation Loss: 1558.9620\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 84\n",
      "Training Loss: 2626.6825\n",
      "Validation Loss: 1539.1021\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 84\n",
      "------------------------------------------------------------\n",
      "Epoch 85\n",
      "Training Loss: 2607.8401\n",
      "Validation Loss: 1584.6379\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 86\n",
      "Training Loss: 2556.5859\n",
      "Validation Loss: 1474.6392\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 86\n",
      "------------------------------------------------------------\n",
      "Epoch 87\n",
      "Training Loss: 2582.7343\n",
      "Validation Loss: 1535.4782\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 88\n",
      "Training Loss: 2544.7777\n",
      "Validation Loss: 1469.0093\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 88\n",
      "------------------------------------------------------------\n",
      "Epoch 89\n",
      "Training Loss: 2527.1247\n",
      "Validation Loss: 1435.4586\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 89\n",
      "------------------------------------------------------------\n",
      "Epoch 90\n",
      "Training Loss: 2485.6449\n",
      "Validation Loss: 1528.4486\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 91\n",
      "Training Loss: 2513.8728\n",
      "Validation Loss: 1422.4641\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 91\n",
      "------------------------------------------------------------\n",
      "Epoch 92\n",
      "Training Loss: 2465.6065\n",
      "Validation Loss: 1458.0242\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 93\n",
      "Training Loss: 2451.1295\n",
      "Validation Loss: 1516.1068\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 94\n",
      "Training Loss: 2469.5000\n",
      "Validation Loss: 1423.9611\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 95\n",
      "Training Loss: 2408.6807\n",
      "Validation Loss: 1399.0319\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 95\n",
      "------------------------------------------------------------\n",
      "Epoch 96\n",
      "Training Loss: 2394.6303\n",
      "Validation Loss: 1447.7260\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 97\n",
      "Training Loss: 2392.2171\n",
      "Validation Loss: 1368.9333\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 97\n",
      "------------------------------------------------------------\n",
      "Epoch 98\n",
      "Training Loss: 2405.1624\n",
      "Validation Loss: 1358.1647\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 98\n",
      "------------------------------------------------------------\n",
      "Epoch 99\n",
      "Training Loss: 2345.9003\n",
      "Validation Loss: 1405.1637\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 100\n",
      "Training Loss: 2338.8180\n",
      "Validation Loss: 1308.8918\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 100\n",
      "------------------------------------------------------------\n",
      "Epoch 101\n",
      "Training Loss: 2305.0101\n",
      "Validation Loss: 1351.2928\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 102\n",
      "Training Loss: 2308.7735\n",
      "Validation Loss: 1337.0375\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 103\n",
      "Training Loss: 2336.6631\n",
      "Validation Loss: 1332.9554\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 104\n",
      "Training Loss: 2276.5875\n",
      "Validation Loss: 1297.5804\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 104\n",
      "------------------------------------------------------------\n",
      "Epoch 105\n",
      "Training Loss: 2281.3247\n",
      "Validation Loss: 1338.6506\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 106\n",
      "Training Loss: 2257.7747\n",
      "Validation Loss: 1356.5311\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 107\n",
      "Training Loss: 2242.4571\n",
      "Validation Loss: 1302.7406\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 108\n",
      "Training Loss: 2244.0254\n",
      "Validation Loss: 1264.5011\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 108\n",
      "------------------------------------------------------------\n",
      "Epoch 109\n",
      "Training Loss: 2204.3360\n",
      "Validation Loss: 1241.9723\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 109\n",
      "------------------------------------------------------------\n",
      "Epoch 110\n",
      "Training Loss: 2180.1291\n",
      "Validation Loss: 1215.5535\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 110\n",
      "------------------------------------------------------------\n",
      "Epoch 111\n",
      "Training Loss: 2186.2957\n",
      "Validation Loss: 1302.0930\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 112\n",
      "Training Loss: 2193.1616\n",
      "Validation Loss: 1223.2577\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 113\n",
      "Training Loss: 2182.4625\n",
      "Validation Loss: 1291.5281\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 114\n",
      "Training Loss: 2180.7055\n",
      "Validation Loss: 1202.1258\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 114\n",
      "------------------------------------------------------------\n",
      "Epoch 115\n",
      "Training Loss: 2124.0048\n",
      "Validation Loss: 1249.7421\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 116\n",
      "Training Loss: 2122.7663\n",
      "Validation Loss: 1241.2553\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 117\n",
      "Training Loss: 2120.1177\n",
      "Validation Loss: 1161.1268\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 117\n",
      "------------------------------------------------------------\n",
      "Epoch 118\n",
      "Training Loss: 2093.1366\n",
      "Validation Loss: 1189.0271\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 119\n",
      "Training Loss: 2069.7611\n",
      "Validation Loss: 1167.1259\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 120\n",
      "Training Loss: 2082.4096\n",
      "Validation Loss: 1145.1528\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 120\n",
      "------------------------------------------------------------\n",
      "Epoch 121\n",
      "Training Loss: 2082.8415\n",
      "Validation Loss: 1117.6069\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 121\n",
      "------------------------------------------------------------\n",
      "Epoch 122\n",
      "Training Loss: 2036.6493\n",
      "Validation Loss: 1207.9509\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 123\n",
      "Training Loss: 2031.4007\n",
      "Validation Loss: 1195.1709\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 124\n",
      "Training Loss: 2019.4980\n",
      "Validation Loss: 1110.3615\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 124\n",
      "------------------------------------------------------------\n",
      "Epoch 125\n",
      "Training Loss: 1987.6205\n",
      "Validation Loss: 1154.5465\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 126\n",
      "Training Loss: 1997.2859\n",
      "Validation Loss: 1189.4617\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 127\n",
      "Training Loss: 1957.9305\n",
      "Validation Loss: 1098.0444\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 127\n",
      "------------------------------------------------------------\n",
      "Epoch 128\n",
      "Training Loss: 1983.6023\n",
      "Validation Loss: 1121.9838\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 129\n",
      "Training Loss: 1963.9497\n",
      "Validation Loss: 1088.6995\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 129\n",
      "------------------------------------------------------------\n",
      "Epoch 130\n",
      "Training Loss: 1934.7459\n",
      "Validation Loss: 1093.2350\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 131\n",
      "Training Loss: 1925.5524\n",
      "Validation Loss: 1102.5154\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 132\n",
      "Training Loss: 1929.7016\n",
      "Validation Loss: 1078.8670\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 132\n",
      "------------------------------------------------------------\n",
      "Epoch 133\n",
      "Training Loss: 1925.0495\n",
      "Validation Loss: 1047.9515\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 133\n",
      "------------------------------------------------------------\n",
      "Epoch 134\n",
      "Training Loss: 1933.0901\n",
      "Validation Loss: 1086.7216\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 135\n",
      "Training Loss: 1884.3528\n",
      "Validation Loss: 1017.2640\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 135\n",
      "------------------------------------------------------------\n",
      "Epoch 136\n",
      "Training Loss: 1866.8311\n",
      "Validation Loss: 1042.5101\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 137\n",
      "Training Loss: 1892.8356\n",
      "Validation Loss: 1045.2727\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 138\n",
      "Training Loss: 1854.3627\n",
      "Validation Loss: 1046.3911\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 139\n",
      "Training Loss: 1820.8981\n",
      "Validation Loss: 1080.9449\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 140\n",
      "Training Loss: 1845.4573\n",
      "Validation Loss: 1034.8470\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 141\n",
      "Training Loss: 1855.4047\n",
      "Validation Loss: 1022.2806\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 142\n",
      "Training Loss: 1679.3254\n",
      "Validation Loss: 941.8597\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 142\n",
      "------------------------------------------------------------\n",
      "Epoch 143\n",
      "Training Loss: 1648.4622\n",
      "Validation Loss: 926.4994\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 143\n",
      "------------------------------------------------------------\n",
      "Epoch 144\n",
      "Training Loss: 1608.3561\n",
      "Validation Loss: 901.9428\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 144\n",
      "------------------------------------------------------------\n",
      "Epoch 145\n",
      "Training Loss: 1606.0757\n",
      "Validation Loss: 921.9676\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 146\n",
      "Training Loss: 1606.3039\n",
      "Validation Loss: 910.6939\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 147\n",
      "Training Loss: 1596.3254\n",
      "Validation Loss: 909.3674\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 148\n",
      "Training Loss: 1578.5214\n",
      "Validation Loss: 912.3599\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 149\n",
      "Training Loss: 1563.6714\n",
      "Validation Loss: 884.0301\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 149\n",
      "------------------------------------------------------------\n",
      "Epoch 150\n",
      "Training Loss: 1550.0359\n",
      "Validation Loss: 875.3605\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 150\n",
      "------------------------------------------------------------\n",
      "Epoch 151\n",
      "Training Loss: 1557.4236\n",
      "Validation Loss: 882.1732\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 152\n",
      "Training Loss: 1561.4560\n",
      "Validation Loss: 922.6016\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 153\n",
      "Training Loss: 1539.0276\n",
      "Validation Loss: 872.3681\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 153\n",
      "------------------------------------------------------------\n",
      "Epoch 154\n",
      "Training Loss: 1548.6964\n",
      "Validation Loss: 875.2100\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 155\n",
      "Training Loss: 1544.5475\n",
      "Validation Loss: 882.2639\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 156\n",
      "Training Loss: 1535.9216\n",
      "Validation Loss: 865.8743\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 156\n",
      "------------------------------------------------------------\n",
      "Epoch 157\n",
      "Training Loss: 1544.3971\n",
      "Validation Loss: 880.7703\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 158\n",
      "Training Loss: 1537.6347\n",
      "Validation Loss: 890.9943\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 159\n",
      "Training Loss: 1514.2105\n",
      "Validation Loss: 846.2583\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 159\n",
      "------------------------------------------------------------\n",
      "Epoch 160\n",
      "Training Loss: 1512.5753\n",
      "Validation Loss: 866.7529\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 161\n",
      "Training Loss: 1489.2720\n",
      "Validation Loss: 874.2706\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 162\n",
      "Training Loss: 1508.7332\n",
      "Validation Loss: 855.3682\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 163\n",
      "Training Loss: 1488.7111\n",
      "Validation Loss: 840.9208\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 163\n",
      "------------------------------------------------------------\n",
      "Epoch 164\n",
      "Training Loss: 1509.6913\n",
      "Validation Loss: 836.3463\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 164\n",
      "------------------------------------------------------------\n",
      "Epoch 165\n",
      "Training Loss: 1503.0063\n",
      "Validation Loss: 830.6618\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 165\n",
      "------------------------------------------------------------\n",
      "Epoch 166\n",
      "Training Loss: 1477.8469\n",
      "Validation Loss: 832.7440\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 167\n",
      "Training Loss: 1474.9897\n",
      "Validation Loss: 846.9182\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 168\n",
      "Training Loss: 1473.8974\n",
      "Validation Loss: 828.4340\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 168\n",
      "------------------------------------------------------------\n",
      "Epoch 169\n",
      "Training Loss: 1480.9693\n",
      "Validation Loss: 828.6006\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 170\n",
      "Training Loss: 1456.7257\n",
      "Validation Loss: 822.8726\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 170\n",
      "------------------------------------------------------------\n",
      "Epoch 171\n",
      "Training Loss: 1465.1824\n",
      "Validation Loss: 810.5895\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 171\n",
      "------------------------------------------------------------\n",
      "Epoch 172\n",
      "Training Loss: 1459.5066\n",
      "Validation Loss: 807.9318\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 172\n",
      "------------------------------------------------------------\n",
      "Epoch 173\n",
      "Training Loss: 1455.6673\n",
      "Validation Loss: 805.0932\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 173\n",
      "------------------------------------------------------------\n",
      "Epoch 174\n",
      "Training Loss: 1456.7865\n",
      "Validation Loss: 824.5762\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 175\n",
      "Training Loss: 1444.1371\n",
      "Validation Loss: 799.9174\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 175\n",
      "------------------------------------------------------------\n",
      "Epoch 176\n",
      "Training Loss: 1428.8605\n",
      "Validation Loss: 823.7389\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 177\n",
      "Training Loss: 1450.3923\n",
      "Validation Loss: 815.0974\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 178\n",
      "Training Loss: 1428.0219\n",
      "Validation Loss: 802.7378\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 179\n",
      "Training Loss: 1438.0014\n",
      "Validation Loss: 793.4133\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 179\n",
      "------------------------------------------------------------\n",
      "Epoch 180\n",
      "Training Loss: 1428.0388\n",
      "Validation Loss: 790.1723\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 180\n",
      "------------------------------------------------------------\n",
      "Epoch 181\n",
      "Training Loss: 1422.8488\n",
      "Validation Loss: 786.6673\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 181\n",
      "------------------------------------------------------------\n",
      "Epoch 182\n",
      "Training Loss: 1428.9050\n",
      "Validation Loss: 832.2495\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 183\n",
      "Training Loss: 1417.7656\n",
      "Validation Loss: 806.9125\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 184\n",
      "Training Loss: 1419.0126\n",
      "Validation Loss: 795.4429\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 185\n",
      "Training Loss: 1400.2471\n",
      "Validation Loss: 775.2239\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 185\n",
      "------------------------------------------------------------\n",
      "Epoch 186\n",
      "Training Loss: 1410.4280\n",
      "Validation Loss: 791.0430\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 187\n",
      "Training Loss: 1388.3131\n",
      "Validation Loss: 773.0143\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 187\n",
      "------------------------------------------------------------\n",
      "Epoch 188\n",
      "Training Loss: 1393.1050\n",
      "Validation Loss: 779.4963\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 189\n",
      "Training Loss: 1396.4269\n",
      "Validation Loss: 773.7681\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 190\n",
      "Training Loss: 1378.7494\n",
      "Validation Loss: 765.9069\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 190\n",
      "------------------------------------------------------------\n",
      "Epoch 191\n",
      "Training Loss: 1387.5597\n",
      "Validation Loss: 779.6901\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 192\n",
      "Training Loss: 1388.3826\n",
      "Validation Loss: 784.8016\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 193\n",
      "Training Loss: 1372.5778\n",
      "Validation Loss: 762.9428\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 193\n",
      "------------------------------------------------------------\n",
      "Epoch 194\n",
      "Training Loss: 1371.8996\n",
      "Validation Loss: 755.2923\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 194\n",
      "------------------------------------------------------------\n",
      "Epoch 195\n",
      "Training Loss: 1382.8671\n",
      "Validation Loss: 773.3638\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 196\n",
      "Training Loss: 1365.2759\n",
      "Validation Loss: 796.8219\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 197\n",
      "Training Loss: 1362.6077\n",
      "Validation Loss: 764.3584\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 198\n",
      "Training Loss: 1375.9954\n",
      "Validation Loss: 752.5218\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 198\n",
      "------------------------------------------------------------\n",
      "Epoch 199\n",
      "Training Loss: 1370.6916\n",
      "Validation Loss: 754.1209\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 200\n",
      "Training Loss: 1368.6335\n",
      "Validation Loss: 752.8469\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 201\n",
      "Training Loss: 1352.1925\n",
      "Validation Loss: 767.8503\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 202\n",
      "Training Loss: 1360.7502\n",
      "Validation Loss: 777.9666\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 203\n",
      "Training Loss: 1339.9093\n",
      "Validation Loss: 761.9108\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 204\n",
      "Training Loss: 1336.7566\n",
      "Validation Loss: 773.9388\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 205\n",
      "Training Loss: 1291.8115\n",
      "Validation Loss: 715.8308\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 205\n",
      "------------------------------------------------------------\n",
      "Epoch 206\n",
      "Training Loss: 1275.5754\n",
      "Validation Loss: 716.4121\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 207\n",
      "Training Loss: 1268.0501\n",
      "Validation Loss: 716.7854\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 208\n",
      "Training Loss: 1259.1458\n",
      "Validation Loss: 724.2209\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 209\n",
      "Training Loss: 1255.0549\n",
      "Validation Loss: 711.2422\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 209\n",
      "------------------------------------------------------------\n",
      "Epoch 210\n",
      "Training Loss: 1260.3653\n",
      "Validation Loss: 709.2422\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 210\n",
      "------------------------------------------------------------\n",
      "Epoch 211\n",
      "Training Loss: 1250.9619\n",
      "Validation Loss: 707.5046\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 211\n",
      "------------------------------------------------------------\n",
      "Epoch 212\n",
      "Training Loss: 1254.1288\n",
      "Validation Loss: 708.1277\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 213\n",
      "Training Loss: 1254.9281\n",
      "Validation Loss: 705.3241\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 213\n",
      "------------------------------------------------------------\n",
      "Epoch 214\n",
      "Training Loss: 1239.8712\n",
      "Validation Loss: 718.1609\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 215\n",
      "Training Loss: 1232.0311\n",
      "Validation Loss: 705.1756\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 215\n",
      "------------------------------------------------------------\n",
      "Epoch 216\n",
      "Training Loss: 1235.3282\n",
      "Validation Loss: 703.5702\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 216\n",
      "------------------------------------------------------------\n",
      "Epoch 217\n",
      "Training Loss: 1210.7786\n",
      "Validation Loss: 697.3101\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 217\n",
      "------------------------------------------------------------\n",
      "Epoch 218\n",
      "Training Loss: 1216.1280\n",
      "Validation Loss: 697.7025\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 219\n",
      "Training Loss: 1229.5861\n",
      "Validation Loss: 699.3778\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 220\n",
      "Training Loss: 1220.3117\n",
      "Validation Loss: 692.5497\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 220\n",
      "------------------------------------------------------------\n",
      "Epoch 221\n",
      "Training Loss: 1217.9801\n",
      "Validation Loss: 699.0397\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 222\n",
      "Training Loss: 1227.9590\n",
      "Validation Loss: 696.7259\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 223\n",
      "Training Loss: 1207.4286\n",
      "Validation Loss: 705.3358\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 224\n",
      "Training Loss: 1234.8555\n",
      "Validation Loss: 690.7155\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 224\n",
      "------------------------------------------------------------\n",
      "Epoch 225\n",
      "Training Loss: 1206.2390\n",
      "Validation Loss: 704.1802\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 226\n",
      "Training Loss: 1221.3509\n",
      "Validation Loss: 686.7264\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 226\n",
      "------------------------------------------------------------\n",
      "Epoch 227\n",
      "Training Loss: 1197.5354\n",
      "Validation Loss: 682.9488\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 227\n",
      "------------------------------------------------------------\n",
      "Epoch 228\n",
      "Training Loss: 1220.3576\n",
      "Validation Loss: 687.7010\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 229\n",
      "Training Loss: 1206.7676\n",
      "Validation Loss: 687.0415\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 230\n",
      "Training Loss: 1211.3312\n",
      "Validation Loss: 685.7914\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 231\n",
      "Training Loss: 1192.9996\n",
      "Validation Loss: 684.8261\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 232\n",
      "Training Loss: 1220.7516\n",
      "Validation Loss: 680.3873\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 232\n",
      "------------------------------------------------------------\n",
      "Epoch 233\n",
      "Training Loss: 1192.6028\n",
      "Validation Loss: 682.2722\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 234\n",
      "Training Loss: 1199.3233\n",
      "Validation Loss: 672.2152\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 234\n",
      "------------------------------------------------------------\n",
      "Epoch 235\n",
      "Training Loss: 1200.9038\n",
      "Validation Loss: 673.1516\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 236\n",
      "Training Loss: 1204.1918\n",
      "Validation Loss: 672.1347\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 236\n",
      "------------------------------------------------------------\n",
      "Epoch 237\n",
      "Training Loss: 1203.9520\n",
      "Validation Loss: 675.3936\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 238\n",
      "Training Loss: 1186.7036\n",
      "Validation Loss: 673.7511\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 239\n",
      "Training Loss: 1201.5051\n",
      "Validation Loss: 669.8673\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 239\n",
      "------------------------------------------------------------\n",
      "Epoch 240\n",
      "Training Loss: 1180.2588\n",
      "Validation Loss: 672.5365\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 241\n",
      "Training Loss: 1196.4552\n",
      "Validation Loss: 677.4885\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 242\n",
      "Training Loss: 1194.8902\n",
      "Validation Loss: 673.2690\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 243\n",
      "Training Loss: 1198.8117\n",
      "Validation Loss: 681.3391\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 244\n",
      "Training Loss: 1179.3595\n",
      "Validation Loss: 666.4207\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 244\n",
      "------------------------------------------------------------\n",
      "Epoch 245\n",
      "Training Loss: 1185.8390\n",
      "Validation Loss: 664.6003\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 245\n",
      "------------------------------------------------------------\n",
      "Epoch 246\n",
      "Training Loss: 1170.5628\n",
      "Validation Loss: 658.8284\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 246\n",
      "------------------------------------------------------------\n",
      "Epoch 247\n",
      "Training Loss: 1174.3786\n",
      "Validation Loss: 658.3963\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 247\n",
      "------------------------------------------------------------\n",
      "Epoch 248\n",
      "Training Loss: 1180.1527\n",
      "Validation Loss: 654.8912\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 248\n",
      "------------------------------------------------------------\n",
      "Epoch 249\n",
      "Training Loss: 1187.7803\n",
      "Validation Loss: 660.3777\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 250\n",
      "Training Loss: 1172.8548\n",
      "Validation Loss: 653.5362\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 250\n",
      "------------------------------------------------------------\n",
      "Epoch 251\n",
      "Training Loss: 1182.8493\n",
      "Validation Loss: 663.9905\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 252\n",
      "Training Loss: 1162.6420\n",
      "Validation Loss: 663.5290\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 253\n",
      "Training Loss: 1166.5798\n",
      "Validation Loss: 654.6753\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 254\n",
      "Training Loss: 1179.2697\n",
      "Validation Loss: 656.5230\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 255\n",
      "Training Loss: 1160.7467\n",
      "Validation Loss: 651.5059\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 255\n",
      "------------------------------------------------------------\n",
      "Epoch 256\n",
      "Training Loss: 1170.9299\n",
      "Validation Loss: 651.0952\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 256\n",
      "------------------------------------------------------------\n",
      "Epoch 257\n",
      "Training Loss: 1159.0713\n",
      "Validation Loss: 650.0723\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 257\n",
      "------------------------------------------------------------\n",
      "Epoch 258\n",
      "Training Loss: 1151.7967\n",
      "Validation Loss: 652.8799\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 259\n",
      "Training Loss: 1164.5269\n",
      "Validation Loss: 653.8064\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 260\n",
      "Training Loss: 1150.5787\n",
      "Validation Loss: 654.8177\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 261\n",
      "Training Loss: 1140.3056\n",
      "Validation Loss: 646.5200\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 261\n",
      "------------------------------------------------------------\n",
      "Epoch 262\n",
      "Training Loss: 1178.0797\n",
      "Validation Loss: 651.1284\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 263\n",
      "Training Loss: 1152.9661\n",
      "Validation Loss: 643.7234\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 263\n",
      "------------------------------------------------------------\n",
      "Epoch 264\n",
      "Training Loss: 1150.4354\n",
      "Validation Loss: 644.0309\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 265\n",
      "Training Loss: 1156.6419\n",
      "Validation Loss: 653.7617\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 266\n",
      "Training Loss: 1157.9687\n",
      "Validation Loss: 644.5314\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 267\n",
      "Training Loss: 1155.6524\n",
      "Validation Loss: 650.5153\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 268\n",
      "Training Loss: 1155.7142\n",
      "Validation Loss: 643.9427\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 269\n",
      "Training Loss: 1148.7859\n",
      "Validation Loss: 639.7859\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 269\n",
      "------------------------------------------------------------\n",
      "Epoch 270\n",
      "Training Loss: 1144.3066\n",
      "Validation Loss: 635.2935\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 270\n",
      "------------------------------------------------------------\n",
      "Epoch 271\n",
      "Training Loss: 1155.1795\n",
      "Validation Loss: 637.9669\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 272\n",
      "Training Loss: 1149.2565\n",
      "Validation Loss: 635.0862\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 272\n",
      "------------------------------------------------------------\n",
      "Epoch 273\n",
      "Training Loss: 1129.0853\n",
      "Validation Loss: 648.4278\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 274\n",
      "Training Loss: 1132.6898\n",
      "Validation Loss: 633.8061\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 274\n",
      "------------------------------------------------------------\n",
      "Epoch 275\n",
      "Training Loss: 1143.8325\n",
      "Validation Loss: 643.6708\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 276\n",
      "Training Loss: 1143.3669\n",
      "Validation Loss: 640.5905\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 277\n",
      "Training Loss: 1134.5503\n",
      "Validation Loss: 639.9608\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 278\n",
      "Training Loss: 1135.4997\n",
      "Validation Loss: 637.8848\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 279\n",
      "Training Loss: 1139.7347\n",
      "Validation Loss: 628.0566\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 279\n",
      "------------------------------------------------------------\n",
      "Epoch 280\n",
      "Training Loss: 1127.8025\n",
      "Validation Loss: 632.9771\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 281\n",
      "Training Loss: 1142.1987\n",
      "Validation Loss: 639.5006\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 282\n",
      "Training Loss: 1135.0949\n",
      "Validation Loss: 629.2691\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 283\n",
      "Training Loss: 1133.4505\n",
      "Validation Loss: 626.6686\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 283\n",
      "------------------------------------------------------------\n",
      "Epoch 284\n",
      "Training Loss: 1129.0713\n",
      "Validation Loss: 626.3204\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 284\n",
      "------------------------------------------------------------\n",
      "Epoch 285\n",
      "Training Loss: 1121.3574\n",
      "Validation Loss: 625.4015\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 285\n",
      "------------------------------------------------------------\n",
      "Epoch 286\n",
      "Training Loss: 1146.0675\n",
      "Validation Loss: 627.7032\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 287\n",
      "Training Loss: 1120.4578\n",
      "Validation Loss: 621.8782\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 287\n",
      "------------------------------------------------------------\n",
      "Epoch 288\n",
      "Training Loss: 1108.6287\n",
      "Validation Loss: 625.2076\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 289\n",
      "Training Loss: 1116.2747\n",
      "Validation Loss: 628.5100\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 290\n",
      "Training Loss: 1111.5048\n",
      "Validation Loss: 618.2560\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 290\n",
      "------------------------------------------------------------\n",
      "Epoch 291\n",
      "Training Loss: 1123.3209\n",
      "Validation Loss: 623.8123\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 292\n",
      "Training Loss: 1111.1441\n",
      "Validation Loss: 614.6180\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 292\n",
      "------------------------------------------------------------\n",
      "Epoch 293\n",
      "Training Loss: 1111.9702\n",
      "Validation Loss: 616.8772\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 294\n",
      "Training Loss: 1107.7098\n",
      "Validation Loss: 637.0521\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 295\n",
      "Training Loss: 1105.9030\n",
      "Validation Loss: 619.3758\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 296\n",
      "Training Loss: 1113.0522\n",
      "Validation Loss: 616.2614\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 297\n",
      "Training Loss: 1113.6736\n",
      "Validation Loss: 613.8834\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 297\n",
      "------------------------------------------------------------\n",
      "Epoch 298\n",
      "Training Loss: 1108.5016\n",
      "Validation Loss: 613.5219\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 298\n",
      "------------------------------------------------------------\n",
      "Epoch 299\n",
      "Training Loss: 1114.0064\n",
      "Validation Loss: 626.0570\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 300\n",
      "Training Loss: 1115.3199\n",
      "Validation Loss: 614.0886\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # データの読み込み\n",
    "    data_pairs = [\n",
    "        #\n",
    "        # 第三回収集データ\n",
    "        #\n",
    "        # # 立ちっぱなし\n",
    "         ('./data/20241115test3/Opti-track/Take 2024-11-15 03.20.00 PM.csv',\n",
    "          './data/20241115test3/insoleSensor/20241115_152500_left.csv',\n",
    "          './data/20241115test3/insoleSensor/20241115_152500_right.csv'),\n",
    "        # お辞儀\n",
    "        ('./data/20241115test3/Opti-track/Take 2024-11-15 03.26.00 PM.csv',\n",
    "         './data/20241115test3/insoleSensor/20241115_153100_left.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_153100_right.csv'),\n",
    "        # 体の横の傾け\n",
    "        ('./data/20241115test3/Opti-track/Take 2024-11-15 03.32.00 PM.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_153700_left.csv', \n",
    "        './data/20241115test3/insoleSensor/20241115_153700_right.csv'),\n",
    "        # 立つ座る\n",
    "        ('./data/20241115test3/Opti-track/Take 2024-11-15 03.38.00 PM.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_154300_left.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_154300_right.csv'),\n",
    "        # スクワット\n",
    "        ('./data/20241115test3/Opti-track/Take 2024-11-15 03.44.00 PM.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_154900_left.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_154900_right.csv'),\n",
    "         # 総合(test3)\n",
    "        #('./data/20241115test3/Opti-track/Take 2024-11-15 03.50.00 PM.csv', \n",
    "        # './data/20241115test3/insoleSensor/20241115_155500_left.csv', \n",
    "        # './data/20241115test3/insoleSensor/20241115_155500_right.csv'),\n",
    "\n",
    "        # 釘宮くん\n",
    "        ('./data/20241212test4/Opti-track/Take 2024-12-12 03.06.59 PM.csv',\n",
    "         './data/20241212test4/insoleSensor/20241212_152700_left.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_152700_right.csv'),\n",
    "        # 百田くん\n",
    "        ('./data/20241212test4/Opti-track/Take 2024-12-12 03.45.00 PM.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_160501_left.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_160501_right.csv'),\n",
    "        # # # # 渡辺(me)\n",
    "         ('./data/20241212test4/Opti-track/Take 2024-12-12 04.28.00 PM.csv', \n",
    "          './data/20241212test4/insoleSensor/20241212_164800_left.csv', \n",
    "          './data/20241212test4/insoleSensor/20241212_164800_right.csv'),\n",
    "        # にるぱむさん\n",
    "        ('./data/20241212test4/Opti-track/Take 2024-12-12 05.17.59 PM.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_173800_left.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_173800_right.csv')\n",
    "    ]\n",
    "    \n",
    "    # データの読み込みと結合\n",
    "    seq_length=3\n",
    "    window_size=1\n",
    "    skeleton_data, pressure_data_left, pressure_data_right, decoder_input, skeleton_label = load_and_combine_data(data_pairs,seq_length,window_size)\n",
    "    \n",
    "    # numpy配列に変換\n",
    "    skeleton_data = skeleton_data.to_numpy()\n",
    "    decoder_input = decoder_input\n",
    "\n",
    "    # 圧力、回転、加速度データの前処理\n",
    "    input_features, sensor_scalers = preprocess_pressure_data(\n",
    "        pressure_data_left,\n",
    "        pressure_data_right\n",
    "    )\n",
    "    print(input_features.shape)\n",
    "    \n",
    "    # データの分割\n",
    "    train_input, val_input, train_skeleton, val_skeleton, train_decoder_input, val_decoder_input = train_test_split(\n",
    "        input_features, \n",
    "        skeleton_label,\n",
    "        decoder_input,\n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    print(train_decoder_input.shape)\n",
    "    print(val_decoder_input.shape)\n",
    "    # デバイスの設定\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # モデルのパラメータ設定\n",
    "    input_dim = input_features.shape[1]  # 圧力+回転+加速度の合計次元数\n",
    "    d_model = 512\n",
    "    nhead = 8\n",
    "    num_encoder_layers = 6\n",
    "    num_joints = skeleton_data.shape[1] // 3  # 3D座標なので3で割る\n",
    "    dropout = 0.1\n",
    "    batch_size = 128\n",
    "\n",
    "    # データローダーの設定\n",
    "    train_dataset = PressureSkeletonDataset(train_input, train_skeleton, train_decoder_input)\n",
    "    val_dataset = PressureSkeletonDataset(val_input, val_skeleton, val_decoder_input)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(\"Checking final training and validation data...\")\n",
    "    print(\"Train input NaN count:\", np.isnan(train_input).sum(), \"Inf count:\", np.isinf(train_input).sum())\n",
    "    print(\"Train skeleton NaN count:\", np.isnan(train_skeleton).sum(), \"Inf count:\", np.isinf(train_skeleton).sum())\n",
    "    \n",
    "    # モデルの初期化\n",
    "    model = EnhancedSkeletonTransformer(\n",
    "        input_dim= input_features.shape[1], # input_dim,\n",
    "        d_model= d_model,\n",
    "        nhead= nhead,\n",
    "        num_encoder_layers= num_encoder_layers,\n",
    "        num_joints=num_joints,\n",
    "        num_dims=3,\n",
    "        dropout=dropout,\n",
    "        seq_length=seq_length,\n",
    "        window_size=window_size\n",
    "    ).to(device)\n",
    "\n",
    "    # 損失関数、オプティマイザ、スケジューラの設定\n",
    "    # criterion = torch.nn.MSELoss()  # 必要に応じてカスタム損失関数に変更可能\n",
    "    criterion = EnhancedSkeletonLoss_WithAngleConstrains(alpha=1.0,gamma=0.5,window_size=window_size)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        weight_decay=0.001,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    # トレーニング実行\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        num_epochs=300,\n",
    "        save_path='./weight/best_skeleton_model.pth',\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # モデルの保存\n",
    "    final_checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'sensor_scalers': sensor_scalers,\n",
    "        # 'skeleton_skaler': skeleton_scaler,\n",
    "        'model_config': {\n",
    "            'input_dim': input_dim,\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_encoder_layers': num_encoder_layers,\n",
    "            'num_joints': num_joints\n",
    "        }\n",
    "    }\n",
    "    torch.save(final_checkpoint, './weight/final_skeleton_model.pth')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56a252-3fe0-4fe8-9633-3a591c44bf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f2617-0947-4329-9b42-b30da55f7ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdb9a7-c658-4d61-83d7-abed0fb4e954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bc1cd-55f8-4ede-941c-8047a6024962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4daf562-d2d9-4dad-8fd2-32b2e4bec770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9320ef-4255-407a-a160-95d9689fd34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c0f76e-b4e9-417a-b49c-b1f20d0b24e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3961, 0.2403, 0.1457, 0.0884, 0.0536, 0.0325, 0.0197, 0.0120, 0.0073,\n",
      "        0.0044])\n"
     ]
    }
   ],
   "source": [
    "def compute_exponential_weights(k, m):\n",
    "    \"\"\"计算指数衰减权重 w_i = exp(-m * i)，并归一化\"\"\"\n",
    "    indices = torch.arange(k)  # 生成 i = 0, 1, ..., k-1\n",
    "    weights = torch.exp(-m * indices)  # 计算 w_i\n",
    "    return weights / weights.sum()  # 归一化，使得所有权重之和为 1\n",
    "\n",
    "# 示例：计算最近 5 个时间步的权重\n",
    "k = 10  # 观察的时间窗口\n",
    "m = 0.5  # 衰减因子（越大表示衰减越快）\n",
    "\n",
    "weights = compute_exponential_weights(k, m)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8749eabc-f3f6-4372-a1b5-1f8e11a89ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSkeletonLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # MSE損失\n",
    "        mse_loss = F.mse_loss(pred, target)\n",
    "        batch_size = int(pred.shape[0])\n",
    "        # 変化量の損失\n",
    "        motion_loss = F.mse_loss(\n",
    "            pred[1:] - pred[:-1],\n",
    "            target[1:] - target[:-1]\n",
    "        )\n",
    "        return self.alpha * mse_loss/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cb7fa6-331d-4708-bec2-79b414891c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction process...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "torch.Size([3004, 1, 63])\n",
      "Making predictions...\n",
      "Prediction shape: torch.Size([2994, 63])\n",
      "Loss: 1.9015629291534424\n",
      "Loss: 23642.900390625\n",
      "\n",
      "Saving predictions...\n",
      "Predictions saved to ./output/predicted_skeleton.csv\n",
      "[[ 5.95058870e+00  8.80480286e+02  1.30444992e+00 ... -8.03655624e+01\n",
      "   8.88666306e+01 -1.08369812e+02]\n",
      " [ 5.28601599e+00  8.74848572e+02  1.16547906e+00 ... -7.58727646e+01\n",
      "   7.67515945e+01 -1.04952759e+02]\n",
      " [ 4.04099131e+00  8.80096069e+02  1.04391479e+00 ... -7.08359909e+01\n",
      "   8.52122803e+01 -1.06465385e+02]\n",
      " ...\n",
      " [ 6.22371960e+00  8.81900146e+02 -6.85157835e-01 ... -7.94677658e+01\n",
      "   9.05017471e+01 -1.47679459e+02]\n",
      " [ 6.23053646e+00  8.81967041e+02 -4.05712605e-01 ... -8.01860428e+01\n",
      "   9.03172913e+01 -1.47116211e+02]\n",
      " [ 6.22929716e+00  8.81923401e+02 -1.83740184e-01 ... -8.06514130e+01\n",
      "   9.01947632e+01 -1.46984451e+02]]\n",
      "Prediction process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_pressure_data(left_data, right_data):\n",
    "    \"\"\"圧力、回転、加速度データの前処理\"\"\"\n",
    "    # 左足データから各種センサー値を抽出\n",
    "    left_pressure = left_data.iloc[:, :35]\n",
    "    left_rotation = left_data.iloc[:, 35:38]\n",
    "    left_accel = left_data.iloc[:, 38:41]\n",
    "\n",
    "    # 右足データから各種センサー値を抽出\n",
    "    right_pressure = right_data.iloc[:, :35]\n",
    "    right_rotation = right_data.iloc[:, 35:38]\n",
    "    right_accel = right_data.iloc[:, 38:41]\n",
    "\n",
    "    # データの結合\n",
    "    pressure_combined = pd.concat([left_pressure, right_pressure], axis=1)\n",
    "    rotation_combined = pd.concat([left_rotation, right_rotation], axis=1)\n",
    "    accel_combined = pd.concat([left_accel, right_accel], axis=1)\n",
    "\n",
    "    # NaN値を補正\n",
    "    pressure_combined = pressure_combined.ffill().bfill()\n",
    "    rotation_combined = rotation_combined.ffill().bfill()\n",
    "    accel_combined = accel_combined.ffill().bfill()\n",
    "\n",
    "    # 移動平均フィルタの適用\n",
    "    window_size = 3\n",
    "    pressure_combined = pressure_combined.rolling(window=window_size, center=True).mean()\n",
    "    rotation_combined = rotation_combined.rolling(window=window_size, center=True).mean()\n",
    "    accel_combined = accel_combined.rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    # NaN値を補間\n",
    "    pressure_combined = pressure_combined.ffill().bfill()\n",
    "    rotation_combined = rotation_combined.ffill().bfill()\n",
    "    accel_combined = accel_combined.ffill().bfill()\n",
    "\n",
    "    # 正規化と標準化\n",
    "    pressure_normalizer = MinMaxScaler()\n",
    "    rotation_normalizer = MinMaxScaler()\n",
    "    accel_normalizer = MinMaxScaler()\n",
    "\n",
    "    pressure_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    rotation_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    accel_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    # データの正規化と標準化\n",
    "    pressure_processed = pressure_standardizer.fit_transform(\n",
    "        pressure_normalizer.fit_transform(pressure_combined)\n",
    "    )\n",
    "    rotation_processed = rotation_standardizer.fit_transform(\n",
    "        rotation_normalizer.fit_transform(rotation_combined)\n",
    "    )\n",
    "    accel_processed = accel_standardizer.fit_transform(\n",
    "        accel_normalizer.fit_transform(accel_combined)\n",
    "    )\n",
    "\n",
    "    # 1次微分と2次微分の計算\n",
    "    pressure_grad1 = np.gradient(pressure_processed, axis=0)\n",
    "    pressure_grad2 = np.gradient(pressure_grad1, axis=0)\n",
    "    \n",
    "    rotation_grad1 = np.gradient(rotation_processed, axis=0)\n",
    "    rotation_grad2 = np.gradient(rotation_grad1, axis=0)\n",
    "    \n",
    "    accel_grad1 = np.gradient(accel_processed, axis=0)\n",
    "    accel_grad2 = np.gradient(accel_grad1, axis=0)\n",
    "\n",
    "    # すべての特徴量を結合（246次元になるはず）\n",
    "    input_features = np.concatenate([\n",
    "        pressure_processed,  # 原特徴量\n",
    "        #pressure_grad1,     # 1次微分\n",
    "        #pressure_grad2,     # 2次微分\n",
    "        rotation_processed,\n",
    "        #rotation_grad1,\n",
    "        #rotation_grad2,\n",
    "        accel_processed,\n",
    "        #accel_grad1,\n",
    "        #accel_grad2\n",
    "    ], axis=1)\n",
    "\n",
    "    return input_features\n",
    "\n",
    "def load_and_preprocess_data(file_pairs):\n",
    "    predictions_all = []\n",
    "    \n",
    "    for skeleton_file, left_file, right_file in file_pairs:\n",
    "        skeleton_data = pd.read_csv(skeleton_file)\n",
    "        pressure_data_left = pd.read_csv(left_file)\n",
    "        pressure_data_right = pd.read_csv(right_file)\n",
    "        \n",
    "        input_features = preprocess_pressure_data(pressure_data_left, pressure_data_right)\n",
    "        min_length = min(len(skeleton_data), len(input_features))\n",
    "        \n",
    "        input_features = input_features.iloc[:min_length]\n",
    "        skeleton_data = skeleton_data.iloc[:min_length]\n",
    "        \n",
    "        predictions_all.append((input_features, skeleton_data))\n",
    "    \n",
    "    return predictions_all\n",
    "\n",
    "def predict_skeleton():\n",
    "    try:\n",
    "        # データの読み込みと前処理\n",
    "        skeleton_data = pd.read_csv('./data/20241115test3/Opti-track/Take 2024-11-15 03.50.00 PM.csv')\n",
    "        pressure_data_left = pd.read_csv('./data/20241115test3/insoleSensor/20241115_155500_left.csv', skiprows=1)\n",
    "        pressure_data_right = pd.read_csv('./data/20241115test3/insoleSensor/20241115_155500_right.csv', skiprows=1)\n",
    "\n",
    "        # 入力データの前処理\n",
    "        input_features = preprocess_pressure_data(pressure_data_left, pressure_data_right)\n",
    "        min_length=min(input_features.shape[0],skeleton_data.shape[0])\n",
    " \n",
    "        # 入力の次元数を取得\n",
    "        seq_length = 3\n",
    "        window_size = 1\n",
    "        m=1\n",
    "        input_dim = input_features.shape[1]\n",
    "        num_joints = skeleton_data.shape[1] // 3\n",
    "\n",
    "        # デバイスの設定\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        skeleton_data=torch.FloatTensor(np.array(skeleton_data)[:min_length]).to(device)\n",
    "\n",
    "        # モデルの初期化（固定パラメータを使用）\n",
    "        model = EnhancedSkeletonTransformer(\n",
    "            input_dim=input_dim,\n",
    "            d_model=512,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=6,\n",
    "            num_joints=num_joints,\n",
    "            num_dims=3,\n",
    "            dropout=0.1,\n",
    "            seq_length=seq_length,\n",
    "            window_size=window_size\n",
    "        ).to(device)\n",
    "\n",
    "        # チェックポイントの読み込み（weights_only=Trueを追加）\n",
    "        checkpoint = torch.load('./weight/best_skeleton_model.pth', map_location=device, weights_only=True)\n",
    "        \n",
    "        # モデルの重みを読み込み\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "\n",
    "        action=torch.zeros((min_length+10,window_size,63)).to(device)\n",
    "        num=np.zeros(min_length+10)\n",
    "        print(action.shape)\n",
    "        # 予測の実行\n",
    "        print(\"Making predictions...\")\n",
    "        predictions=torch.zeros(min_length,63).to(device)\n",
    "        with torch.no_grad():\n",
    "            skeleton_last=torch.zeros((seq_length,63))\n",
    "            skeleton_last=skeleton_last.unsqueeze(0).to(device)\n",
    "            for i in range(min_length):\n",
    "                input_tensor = torch.FloatTensor(input_features)[i].to(device)\n",
    "                input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "                \n",
    "                skeleton_last_pos=add_positional_encoding(skeleton_last)\n",
    "                skeleton_predict_seq=model(input_tensor,skeleton_last)\n",
    "                skeleton_predict_seq=skeleton_predict_seq.squeeze(0)\n",
    "                skeleton_predict=torch.zeros(63).to(device)\n",
    "                for j in range(window_size):\n",
    "                    action[i+j,int(num[i+j])]=skeleton_predict_seq[j,:]\n",
    "                    num[i+j]+=1\n",
    "                    #print(f\"j={j},i+j={i+j},num[i+j}]={int(num[i+j])}\")\n",
    "                weights = compute_exponential_weights(int(num[i]), m).to(device)\n",
    "                for j in range(int(num[i])):\n",
    "                    skeleton_predict+=weights[j]*action[i,int(num[i])-1-j]\n",
    "                predictions[i]=skeleton_predict\n",
    "                for j in range(seq_length-1):\n",
    "                    skeleton_last[0,j]=skeleton_last[0,j+1]\n",
    "                skeleton_last[0,seq_length-1]=skeleton_predict\n",
    "        '''\n",
    "        # 予測の実行\n",
    "        print(\"Making predictions...\")\n",
    "        predictions=torch.zeros(min_length,63).to(device)\n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.FloatTensor(input_features).to(device)\n",
    "            input_tensor=input_tensor.to(device)\n",
    "            print(input_tensor.shape,skeleton_data.shape)\n",
    "            predictions=model(input_tensor,skeleton_data)\n",
    "        '''\n",
    "        print(f\"Prediction shape: {predictions.shape}\")\n",
    "        criterion = EnhancedSkeletonLoss(alpha=1.0,)\n",
    "        criterion1 = EnhancedSkeletonLoss_WithAngleConstrains(alpha=1.0,gamma=0.1,window_size=1)\n",
    "        predictionsa = predictions.unsqueeze(1)\n",
    "        skeleton_dataa = skeleton_data.unsqueeze(1)\n",
    "        loss=criterion(predictionsa,skeleton_dataa)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        loss1=criterion1(predictionsa,skeleton_dataa)\n",
    "        print(f\"Loss: {loss1}\")\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        return predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def save_predictions(predictions, output_file='./output/predicted_skeleton.csv'):\n",
    "    try:\n",
    "        # 予測結果をデータフレームに変換\n",
    "        num_joints = predictions.shape[1] // 3\n",
    "        columns = []\n",
    "        for i in range(num_joints):\n",
    "            columns.extend([f'X.{i*2+1}', f'Y.{i*2+1}', f'Z.{i*2+1}'])\n",
    "        \n",
    "        df_predictions = pd.DataFrame(predictions, columns=columns)\n",
    "        df_predictions.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting prediction process...\")\n",
    "        predictions = predict_skeleton()\n",
    "        \n",
    "        print(\"\\nSaving predictions...\")\n",
    "        save_predictions(predictions)\n",
    "        print(predictions)\n",
    "        \n",
    "        print(\"Prediction process completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b55a32-1294-4b56-9928-fe913d0ba81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
