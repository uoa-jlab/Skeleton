{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "332291e0-89e2-4522-827b-404b2614173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f48cde7-172e-4c77-a995-bed443cef989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(data, noise_factor=0.1):\n",
    "    std = torch.std(data, dim=0, keepdim=True)  # 计算原始数据的标准差\n",
    "    noise = torch.randn_like(data) * (std * noise_factor)  # 生成噪声\n",
    "    return data + noise\n",
    "def add_positional_encoding(x):\n",
    "    \"\"\"\n",
    "    x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "    # 初始化位置编码矩阵 (seq_len, d_model)\n",
    "    pe = torch.zeros(seq_len, d_model, device=x.device)\n",
    "\n",
    "    position = torch.arange(seq_len, device=x.device).unsqueeze(1)  # (seq_len, 1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, device=x.device) * (-math.log(10000.0) / d_model))  # (d_model/2)\n",
    "\n",
    "    # 处理嵌入维度为奇数的情况\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term[:(d_model//2)])  # 防止越界\n",
    "\n",
    "    # 添加维度以便广播： (1, seq_len, d_model)\n",
    "    pe = pe.unsqueeze(0)\n",
    "\n",
    "    # 加到输入上 (batch_size, seq_len, d_model)\n",
    "    return x + pe\n",
    "class PressureSkeletonDataset(Dataset):\n",
    "    def __init__(self, pressure_data, skeleton_data, decoder_input):\n",
    "        self.pressure_data = torch.FloatTensor(pressure_data)\n",
    "        self.skeleton_data = torch.FloatTensor(skeleton_data)\n",
    "        self.decoder_input = torch.FloatTensor(decoder_input)\n",
    "    def __len__(self):\n",
    "        return len(self.pressure_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pressure_data[idx], self.skeleton_data[idx],self.decoder_input[idx]    \n",
    "class EnhancedSkeletonTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_joints, num_dims=3, dropout=0.1,seq_length=3,window_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # クラス属性としてnum_jointsを保存\n",
    "        self.num_joints = num_joints\n",
    "        self.num_dims = num_dims\n",
    "        self.seq_length=seq_length\n",
    "        self.window_size=window_size\n",
    "        \n",
    "        # 入力の特徴抽出を強化\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim,d_model)\n",
    "        )\n",
    "        \n",
    "        # より深いTransformerネットワーク\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,  \n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "\n",
    "        self.decoder_feature_extractor = nn.Sequential(\n",
    "            nn.Linear(num_joints*num_dims,d_model)\n",
    "        )\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(d_model*seq_length,d_model*window_size)\n",
    "        )\n",
    "        \n",
    "        # 出力層の強化\n",
    "        self.output_decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.LayerNorm(d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_joints * num_dims)\n",
    "        )\n",
    "        # スケール係数（学習可能パラメータ）\n",
    "        self.output_scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    \n",
    "    def forward(self, x, decoder_input):\n",
    "        batch_size=x.shape[0]\n",
    "        \n",
    "        # 特徴抽出\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.unsqueeze(1)\n",
    "        decoder_input = self.decoder_feature_extractor(decoder_input)\n",
    "        decoder_input = add_positional_encoding(decoder_input)\n",
    "\n",
    "        # Transformer処理\n",
    "        transformer_output = self.transformer_encoder(features)\n",
    "        transformer_output = self.transformer_decoder(decoder_input, transformer_output)\n",
    "        #predict = transformer_output[:,transformer_output.shape[1]-1,:]\n",
    "\n",
    "        predict=transformer_output.reshape(batch_size,-1)\n",
    "        predict_next=self.predict(predict)\n",
    "        predict_next=predict_next.reshape(batch_size,self.window_size,-1)\n",
    "        \n",
    "        # 出力生成とスケーリング\n",
    "        output = self.output_decoder(predict_next)\n",
    "        #output = self.constraint(output)\n",
    "        output = output * self.output_scale  # 出力のスケーリング\n",
    "\n",
    "        \n",
    "        return output\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, save_path, device):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for pressure, skeleton, decoder_input in train_loader:\n",
    "            # データをGPUに移動\n",
    "            pressure = pressure.to(device)\n",
    "            skeleton = skeleton.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_input = add_positional_encoding(decoder_input)\n",
    "            \n",
    "            pressure = add_gaussian_noise(pressure, noise_factor=0.1)\n",
    "            decoder_input =  add_gaussian_noise(decoder_input, noise_factor=0.1)\n",
    "            if torch.rand(1).item() < 0.95:\n",
    "                decoder_input=torch.zeros_like(decoder_input)\n",
    "            outputs = model(pressure,decoder_input)\n",
    "            loss = criterion(outputs,skeleton)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for pressure, skeleton,decoder_input in val_loader:\n",
    "                # データをGPUに移動\n",
    "                pressure = pressure.to(device)\n",
    "                skeleton = skeleton.to(device)\n",
    "                decoder_input = decoder_input.to(device)\n",
    "                \n",
    "                outputs = model(pressure,decoder_input)\n",
    "                loss = criterion(outputs, skeleton)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # 平均損失の計算\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # スケジューラのステップ\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Learning Rate: {current_lr:.6f}')\n",
    "        \n",
    "        # モデルの保存\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, save_path)\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "        \n",
    "        print('-' * 60)\n",
    "class EnhancedSkeletonLoss_WithAngleConstrains(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=0.5, window_size=5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # MSE損失\n",
    "        # 假设 pred 和 target 的原始形状为 [batch_size, window_size * num_joints * 3]\n",
    "        batch_size = int(pred.shape[0])\n",
    "        num_joints = int(pred.shape[2] // 3)\n",
    "        \n",
    "        # 重塑为 [batch_size, window_size, num_joints, 3]\n",
    "        pred_reshaped = pred.view(batch_size, self.window_size, num_joints, 3)\n",
    "        target_reshaped = target.view(batch_size, self.window_size, num_joints, 3)\n",
    "        \n",
    "        # 定义关节点权重，默认全部为 1.0\n",
    "        joint_weights = torch.tensor(0.2, device=pred.device)*torch.ones(num_joints, device=pred.device)\n",
    "        # 对背骨的关节点（索引 0～4）赋予较高权重\n",
    "        for idx in [0, 1, 2, 3, 4]:\n",
    "            joint_weights[idx] = 2.0\n",
    "        # 对两个腿的关节点（例如：一侧 13～16，另一侧 17～20）赋予较高权重\n",
    "        for idx in [13, 14, 15, 16, 17, 18, 19, 20]:\n",
    "            joint_weights[idx] = 2.0\n",
    "        \n",
    "        # 计算加权均方误差\n",
    "        # 先计算每个坐标的平方误差，形状为 [batch_size, window_size, num_joints, 3]\n",
    "        squared_diff = (pred_reshaped - target_reshaped) ** 2\n",
    "        # 对坐标求和得到每个关节的误差，形状为 [batch_size, window_size, num_joints]\n",
    "        squared_diff = squared_diff.sum(dim=-1)\n",
    "        # 将关节点的权重扩展到 [1, 1, num_joints] 后相乘\n",
    "        weighted_squared_diff = squared_diff * joint_weights.view(1, 1, num_joints)\n",
    "        # 平均得到加权的均方误差\n",
    "        mse_loss = weighted_squared_diff.mean()\n",
    "\n",
    "        \n",
    "        eps = 1e-6\n",
    "        angle_loss = 0.0\n",
    "        angle_pairs = [\n",
    "            ((0, 1), (1, 2)),\n",
    "            ((1, 2), (2, 3)),\n",
    "            ((2, 3), (3, 4)),\n",
    "            ((13, 17), (17, 18)),\n",
    "            ((13, 17), (13, 14)),\n",
    "            ((17, 18), (18, 19)),\n",
    "            ((18, 19), (19, 20)),\n",
    "            ((13, 14), (14, 15)),\n",
    "            ((14, 15), (15, 16))\n",
    "        ]\n",
    "        for (bone1, bone2) in angle_pairs:\n",
    "            # 预测向量计算\n",
    "            pred_vec1 = pred_reshaped[:, :, bone1[1], :] - pred_reshaped[:, :, bone1[0], :]\n",
    "            pred_vec2 = pred_reshaped[:, :, bone2[1], :] - pred_reshaped[:, :, bone2[0], :]\n",
    "            dot_pred = (pred_vec1 * pred_vec2).sum(dim=-1)\n",
    "            norm_pred1 = torch.norm(pred_vec1, dim=-1)\n",
    "            norm_pred2 = torch.norm(pred_vec2, dim=-1)\n",
    "            cos_pred = dot_pred / (norm_pred1 * norm_pred2 + eps)\n",
    "            cos_pred = torch.clamp(cos_pred, -1.0, 1.0)\n",
    "            \n",
    "            # 目标向量计算\n",
    "            target_vec1 = target_reshaped[:, :, bone1[1], :] - target_reshaped[:, :, bone1[0], :]\n",
    "            target_vec2 = target_reshaped[:, :, bone2[1], :] - target_reshaped[:, :, bone2[0], :]\n",
    "            dot_target = (target_vec1 * target_vec2).sum(dim=-1)\n",
    "            norm_target1 = torch.norm(target_vec1, dim=-1)\n",
    "            norm_target2 = torch.norm(target_vec2, dim=-1)\n",
    "            cos_target = dot_target / (norm_target1 * norm_target2 + eps)\n",
    "            cos_target = torch.clamp(cos_target, -1.0, 1.0)\n",
    "            \n",
    "            # 直接比较余弦值的差异\n",
    "            angle_loss += F.mse_loss(cos_pred, cos_target)\n",
    "        angle_loss = angle_loss / len(angle_pairs)\n",
    "    \n",
    "        return self.alpha * mse_loss + self.gamma*angle_loss\n",
    "\n",
    "def load_model(model, optimizer, scheduler, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "    return model, optimizer, scheduler, epoch, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "241bcbf2-1b02-4bf2-a569-526eed7aae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_data(file_pairs,seq_length,window_size):\n",
    "    \"\"\"複数のデータセットを読み込んで結合する\"\"\"\n",
    "    all_skeleton_data = []\n",
    "    all_pressure_left = []\n",
    "    all_pressure_right = []\n",
    "    all_decoder_input = []\n",
    "\n",
    "    all_skeleton_label = []\n",
    "    for skeleton_file, left_file, right_file in file_pairs:\n",
    "        skeleton = pd.read_csv(skeleton_file)\n",
    "        left = pd.read_csv(left_file, dtype=float, low_memory=False)\n",
    "        right = pd.read_csv(right_file, dtype=float, low_memory=False)\n",
    "        # データ長を揃える\n",
    "        min_length = min(len(skeleton), len(left), len(right))\n",
    "\n",
    "        num_joints_points=skeleton.shape[1]\n",
    "        decoder_input = np.zeros((min_length,seq_length,num_joints_points))\n",
    "        for i in range(min_length):\n",
    "            for j in range(1,seq_length+1):\n",
    "                if i-j<0:\n",
    "                    continue\n",
    "                else:\n",
    "                    decoder_input[i,seq_length-j]=skeleton.iloc[i-j]\n",
    "\n",
    "        skeleton_label = np.zeros((min_length,window_size,num_joints_points))\n",
    "        for i in range(min_length):\n",
    "            for j in range(window_size):\n",
    "                if i+j<min_length:\n",
    "                    skeleton_label[i,j]=skeleton.iloc[i+j]\n",
    "        \n",
    "        all_skeleton_data.append(skeleton.iloc[:min_length])\n",
    "        all_pressure_left.append(left.iloc[:min_length])\n",
    "        all_pressure_right.append(right.iloc[:min_length])\n",
    "        all_decoder_input.append(decoder_input)\n",
    "        all_skeleton_label.append(skeleton_label)\n",
    "        \n",
    "    return (pd.concat(all_skeleton_data, ignore_index=True),\n",
    "            pd.concat(all_pressure_left, ignore_index=True),\n",
    "            pd.concat(all_pressure_right, ignore_index=True),\n",
    "            np.concatenate(all_decoder_input),\n",
    "            np.concatenate(all_skeleton_label))\n",
    "\n",
    "def preprocess_pressure_data(left_data, right_data):\n",
    "    \"\"\"圧力、回転、加速度データの前処理\"\"\"\n",
    "    \n",
    "    # 左足データから各種センサー値を抽出\n",
    "    left_pressure = left_data.iloc[:, :35]  # 圧力センサーの列を適切に指定\n",
    "    left_rotation = left_data.iloc[:, 35:38]  # 回転データの列を適切に指定\n",
    "    left_accel = left_data.iloc[:, 38:41]  # 加速度データの列を適切に指定\n",
    "\n",
    "    # 右足データから各種センサー値を抽出\n",
    "    right_pressure = right_data.iloc[:, :35]  # 圧力センサーの列を適切に指定\n",
    "    right_rotation = right_data.iloc[:, 35:38]  # 回転データの列を適切に指定\n",
    "    right_accel = right_data.iloc[:, 38:41]  # 加速度データの列を適切に指定\n",
    "\n",
    "    # データの結合(按列（属性）相拼接)\n",
    "    pressure_combined = pd.concat([left_pressure, right_pressure], axis=1)\n",
    "    rotation_combined = pd.concat([left_rotation, right_rotation], axis=1)\n",
    "    accel_combined = pd.concat([left_accel, right_accel], axis=1)\n",
    "\n",
    "    # NaN値を補正\n",
    "    pressure_combined = pressure_combined.fillna(0.0)\n",
    "    rotation_combined = rotation_combined.fillna(0.0)\n",
    "    accel_combined = accel_combined.fillna(0.0)\n",
    "\n",
    "    print(\"Checking pressure data for NaN or Inf...\")\n",
    "    print(\"Pressure NaN count:\", pressure_combined.isna().sum().sum())\n",
    "    print(\"Pressure Inf count:\", np.isinf(pressure_combined).sum().sum())\n",
    "\n",
    "    # 移動平均フィルタの適用\n",
    "    window_size = 3\n",
    "    pressure_combined = pressure_combined.rolling(window=window_size, center=True).mean()\n",
    "    rotation_combined = rotation_combined.rolling(window=window_size, center=True).mean()\n",
    "    accel_combined = accel_combined.rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    # NaN値を前後の値で補間\n",
    "    pressure_combined = pressure_combined.bfill().ffill()\n",
    "    rotation_combined = rotation_combined.bfill().ffill()\n",
    "    accel_combined = accel_combined.bfill().ffill()\n",
    "\n",
    "    # 正規化と標準化のスケーラー初期化\n",
    "    pressure_normalizer = MinMaxScaler()\n",
    "    rotation_normalizer = MinMaxScaler()\n",
    "    accel_normalizer = MinMaxScaler()\n",
    "\n",
    "    pressure_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    rotation_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    accel_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    # データの正規化と標準化\n",
    "    pressure_processed = pressure_standardizer.fit_transform(\n",
    "        pressure_normalizer.fit_transform(pressure_combined)\n",
    "    )\n",
    "    rotation_processed = rotation_standardizer.fit_transform(\n",
    "        rotation_normalizer.fit_transform(rotation_combined)\n",
    "    )\n",
    "    accel_processed = accel_standardizer.fit_transform(\n",
    "        accel_normalizer.fit_transform(accel_combined)\n",
    "    )\n",
    "\n",
    "    # 1次微分と2次微分の計算\n",
    "    pressure_grad1 = np.gradient(pressure_processed, axis=0)\n",
    "    pressure_grad2 = np.gradient(pressure_grad1, axis=0)\n",
    "    \n",
    "    rotation_grad1 = np.gradient(rotation_processed, axis=0)\n",
    "    '''不存在物理意义\n",
    "    rotation_grad2 = np.gradient(rotation_grad1, axis=0)\n",
    "    \n",
    "    accel_grad1 = np.gradient(accel_processed, axis=0)\n",
    "    accel_grad2 = np.gradient(accel_grad1, axis=0)\n",
    "    '''\n",
    "    \n",
    "    # すべての特徴量を結合\n",
    "    input_features = np.concatenate([\n",
    "        pressure_processed,\n",
    "        #pressure_grad1,\n",
    "        #pressure_grad2,\n",
    "        rotation_processed,\n",
    "        #rotation_grad1,\n",
    "        #rotation_grad2,\n",
    "        accel_processed,\n",
    "        #accel_grad1,\n",
    "        #accel_grad2\n",
    "    ], axis=1)\n",
    "\n",
    "    return input_features, {\n",
    "        'pressure': {\n",
    "            'normalizer': pressure_normalizer,\n",
    "            'standardizer': pressure_standardizer\n",
    "        },\n",
    "        'rotation': {\n",
    "            'normalizer': rotation_normalizer,\n",
    "            'standardizer': rotation_standardizer\n",
    "        },\n",
    "        'accel': {\n",
    "            'normalizer': accel_normalizer,\n",
    "            'standardizer': accel_standardizer\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fc114f1-cfac-4145-8f6b-d5b525c80208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking pressure data for NaN or Inf...\n",
      "Pressure NaN count: 0\n",
      "Pressure Inf count: 0\n",
      "(62782, 82)\n",
      "(50225, 3, 63)\n",
      "(12557, 3, 63)\n",
      "Using device: cuda:0\n",
      "Checking final training and validation data...\n",
      "Train input NaN count: 0 Inf count: 0\n",
      "Train skeleton NaN count: 0 Inf count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 919393.0733\n",
      "Validation Loss: 710077.0947\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 1\n",
      "------------------------------------------------------------\n",
      "Epoch 2\n",
      "Training Loss: 461901.7802\n",
      "Validation Loss: 226493.5698\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 2\n",
      "------------------------------------------------------------\n",
      "Epoch 3\n",
      "Training Loss: 93648.5142\n",
      "Validation Loss: 37538.8061\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 3\n",
      "------------------------------------------------------------\n",
      "Epoch 4\n",
      "Training Loss: 34957.9949\n",
      "Validation Loss: 31137.0479\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 4\n",
      "------------------------------------------------------------\n",
      "Epoch 5\n",
      "Training Loss: 26550.4700\n",
      "Validation Loss: 23821.0222\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 5\n",
      "------------------------------------------------------------\n",
      "Epoch 6\n",
      "Training Loss: 24087.5162\n",
      "Validation Loss: 23738.8209\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 6\n",
      "------------------------------------------------------------\n",
      "Epoch 7\n",
      "Training Loss: 24325.5637\n",
      "Validation Loss: 22741.3041\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 7\n",
      "------------------------------------------------------------\n",
      "Epoch 8\n",
      "Training Loss: 23677.9924\n",
      "Validation Loss: 21366.0989\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 8\n",
      "------------------------------------------------------------\n",
      "Epoch 9\n",
      "Training Loss: 20835.8905\n",
      "Validation Loss: 19522.9221\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 9\n",
      "------------------------------------------------------------\n",
      "Epoch 10\n",
      "Training Loss: 19641.6997\n",
      "Validation Loss: 17780.3390\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 10\n",
      "------------------------------------------------------------\n",
      "Epoch 11\n",
      "Training Loss: 17743.7662\n",
      "Validation Loss: 16451.4437\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 11\n",
      "------------------------------------------------------------\n",
      "Epoch 12\n",
      "Training Loss: 16765.0970\n",
      "Validation Loss: 14927.5158\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 12\n",
      "------------------------------------------------------------\n",
      "Epoch 13\n",
      "Training Loss: 15823.7830\n",
      "Validation Loss: 13613.5580\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 13\n",
      "------------------------------------------------------------\n",
      "Epoch 14\n",
      "Training Loss: 14978.6142\n",
      "Validation Loss: 12856.1056\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 14\n",
      "------------------------------------------------------------\n",
      "Epoch 15\n",
      "Training Loss: 14369.4355\n",
      "Validation Loss: 12164.6360\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 15\n",
      "------------------------------------------------------------\n",
      "Epoch 16\n",
      "Training Loss: 13948.9629\n",
      "Validation Loss: 11810.0465\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 16\n",
      "------------------------------------------------------------\n",
      "Epoch 17\n",
      "Training Loss: 13544.8817\n",
      "Validation Loss: 11556.4726\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 17\n",
      "------------------------------------------------------------\n",
      "Epoch 18\n",
      "Training Loss: 13195.8011\n",
      "Validation Loss: 11281.1961\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 18\n",
      "------------------------------------------------------------\n",
      "Epoch 19\n",
      "Training Loss: 12820.3269\n",
      "Validation Loss: 10716.8430\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 19\n",
      "------------------------------------------------------------\n",
      "Epoch 20\n",
      "Training Loss: 12424.6767\n",
      "Validation Loss: 10438.9786\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 20\n",
      "------------------------------------------------------------\n",
      "Epoch 21\n",
      "Training Loss: 12109.8476\n",
      "Validation Loss: 10173.7308\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 21\n",
      "------------------------------------------------------------\n",
      "Epoch 22\n",
      "Training Loss: 11705.8765\n",
      "Validation Loss: 9701.9529\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 22\n",
      "------------------------------------------------------------\n",
      "Epoch 23\n",
      "Training Loss: 11346.3337\n",
      "Validation Loss: 9455.0641\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 23\n",
      "------------------------------------------------------------\n",
      "Epoch 24\n",
      "Training Loss: 11005.3697\n",
      "Validation Loss: 9143.6329\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 24\n",
      "------------------------------------------------------------\n",
      "Epoch 25\n",
      "Training Loss: 10896.8645\n",
      "Validation Loss: 8913.2557\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 25\n",
      "------------------------------------------------------------\n",
      "Epoch 26\n",
      "Training Loss: 10654.1741\n",
      "Validation Loss: 8718.1166\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 26\n",
      "------------------------------------------------------------\n",
      "Epoch 27\n",
      "Training Loss: 10416.2276\n",
      "Validation Loss: 8567.7969\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 27\n",
      "------------------------------------------------------------\n",
      "Epoch 28\n",
      "Training Loss: 10061.2311\n",
      "Validation Loss: 8191.7119\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 28\n",
      "------------------------------------------------------------\n",
      "Epoch 29\n",
      "Training Loss: 9716.7387\n",
      "Validation Loss: 7774.4120\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 29\n",
      "------------------------------------------------------------\n",
      "Epoch 30\n",
      "Training Loss: 9253.9540\n",
      "Validation Loss: 7561.4721\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 30\n",
      "------------------------------------------------------------\n",
      "Epoch 31\n",
      "Training Loss: 8741.7962\n",
      "Validation Loss: 6836.5642\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 31\n",
      "------------------------------------------------------------\n",
      "Epoch 32\n",
      "Training Loss: 8212.8204\n",
      "Validation Loss: 6524.9137\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 32\n",
      "------------------------------------------------------------\n",
      "Epoch 33\n",
      "Training Loss: 7896.1633\n",
      "Validation Loss: 6136.1144\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 33\n",
      "------------------------------------------------------------\n",
      "Epoch 34\n",
      "Training Loss: 7690.8547\n",
      "Validation Loss: 5884.3119\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 34\n",
      "------------------------------------------------------------\n",
      "Epoch 35\n",
      "Training Loss: 7452.7037\n",
      "Validation Loss: 5638.2503\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 35\n",
      "------------------------------------------------------------\n",
      "Epoch 36\n",
      "Training Loss: 7218.0753\n",
      "Validation Loss: 5479.5547\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 36\n",
      "------------------------------------------------------------\n",
      "Epoch 37\n",
      "Training Loss: 7037.5028\n",
      "Validation Loss: 5312.4522\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 37\n",
      "------------------------------------------------------------\n",
      "Epoch 38\n",
      "Training Loss: 6861.2077\n",
      "Validation Loss: 5181.0651\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 38\n",
      "------------------------------------------------------------\n",
      "Epoch 39\n",
      "Training Loss: 6653.1179\n",
      "Validation Loss: 4885.5990\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 39\n",
      "------------------------------------------------------------\n",
      "Epoch 40\n",
      "Training Loss: 6372.6047\n",
      "Validation Loss: 4605.6546\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 40\n",
      "------------------------------------------------------------\n",
      "Epoch 41\n",
      "Training Loss: 6215.2062\n",
      "Validation Loss: 4463.6646\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 41\n",
      "------------------------------------------------------------\n",
      "Epoch 42\n",
      "Training Loss: 5968.8853\n",
      "Validation Loss: 4208.2510\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 42\n",
      "------------------------------------------------------------\n",
      "Epoch 43\n",
      "Training Loss: 5775.9068\n",
      "Validation Loss: 3943.2563\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 43\n",
      "------------------------------------------------------------\n",
      "Epoch 44\n",
      "Training Loss: 5564.8223\n",
      "Validation Loss: 3877.4336\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 44\n",
      "------------------------------------------------------------\n",
      "Epoch 45\n",
      "Training Loss: 5382.2376\n",
      "Validation Loss: 3770.4804\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 45\n",
      "------------------------------------------------------------\n",
      "Epoch 46\n",
      "Training Loss: 5239.0141\n",
      "Validation Loss: 3592.0439\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 46\n",
      "------------------------------------------------------------\n",
      "Epoch 47\n",
      "Training Loss: 5086.6182\n",
      "Validation Loss: 3469.7811\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 47\n",
      "------------------------------------------------------------\n",
      "Epoch 48\n",
      "Training Loss: 4969.2234\n",
      "Validation Loss: 3368.1700\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 48\n",
      "------------------------------------------------------------\n",
      "Epoch 49\n",
      "Training Loss: 4793.8827\n",
      "Validation Loss: 3190.0397\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 49\n",
      "------------------------------------------------------------\n",
      "Epoch 50\n",
      "Training Loss: 4733.3648\n",
      "Validation Loss: 3094.3847\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 50\n",
      "------------------------------------------------------------\n",
      "Epoch 51\n",
      "Training Loss: 4612.4546\n",
      "Validation Loss: 3083.4720\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 51\n",
      "------------------------------------------------------------\n",
      "Epoch 52\n",
      "Training Loss: 4502.7916\n",
      "Validation Loss: 2931.1334\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 52\n",
      "------------------------------------------------------------\n",
      "Epoch 53\n",
      "Training Loss: 4424.9651\n",
      "Validation Loss: 2914.2885\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 53\n",
      "------------------------------------------------------------\n",
      "Epoch 54\n",
      "Training Loss: 4342.6077\n",
      "Validation Loss: 2864.1501\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 54\n",
      "------------------------------------------------------------\n",
      "Epoch 55\n",
      "Training Loss: 4283.5684\n",
      "Validation Loss: 2792.3369\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 55\n",
      "------------------------------------------------------------\n",
      "Epoch 56\n",
      "Training Loss: 4220.9989\n",
      "Validation Loss: 2701.5367\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 56\n",
      "------------------------------------------------------------\n",
      "Epoch 57\n",
      "Training Loss: 4150.2970\n",
      "Validation Loss: 2639.4642\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 57\n",
      "------------------------------------------------------------\n",
      "Epoch 58\n",
      "Training Loss: 4062.3356\n",
      "Validation Loss: 2615.6562\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 58\n",
      "------------------------------------------------------------\n",
      "Epoch 59\n",
      "Training Loss: 3994.5160\n",
      "Validation Loss: 2535.6794\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 59\n",
      "------------------------------------------------------------\n",
      "Epoch 60\n",
      "Training Loss: 3864.5981\n",
      "Validation Loss: 2532.7878\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 60\n",
      "------------------------------------------------------------\n",
      "Epoch 61\n",
      "Training Loss: 3837.7867\n",
      "Validation Loss: 2434.4221\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 61\n",
      "------------------------------------------------------------\n",
      "Epoch 62\n",
      "Training Loss: 3789.5910\n",
      "Validation Loss: 2419.8550\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 62\n",
      "------------------------------------------------------------\n",
      "Epoch 63\n",
      "Training Loss: 3688.5036\n",
      "Validation Loss: 2352.8841\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 63\n",
      "------------------------------------------------------------\n",
      "Epoch 64\n",
      "Training Loss: 3716.3308\n",
      "Validation Loss: 2400.6525\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 65\n",
      "Training Loss: 3576.6055\n",
      "Validation Loss: 2239.8555\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 65\n",
      "------------------------------------------------------------\n",
      "Epoch 66\n",
      "Training Loss: 3523.0176\n",
      "Validation Loss: 2141.0204\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 66\n",
      "------------------------------------------------------------\n",
      "Epoch 67\n",
      "Training Loss: 3454.9010\n",
      "Validation Loss: 2101.1461\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 67\n",
      "------------------------------------------------------------\n",
      "Epoch 68\n",
      "Training Loss: 3418.6821\n",
      "Validation Loss: 2031.9451\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 68\n",
      "------------------------------------------------------------\n",
      "Epoch 69\n",
      "Training Loss: 3360.3530\n",
      "Validation Loss: 2152.3973\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 70\n",
      "Training Loss: 3331.8975\n",
      "Validation Loss: 1948.4089\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 70\n",
      "------------------------------------------------------------\n",
      "Epoch 71\n",
      "Training Loss: 3250.0787\n",
      "Validation Loss: 1919.5323\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 71\n",
      "------------------------------------------------------------\n",
      "Epoch 72\n",
      "Training Loss: 3263.4936\n",
      "Validation Loss: 1973.6024\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 73\n",
      "Training Loss: 3195.3544\n",
      "Validation Loss: 1895.2802\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 73\n",
      "------------------------------------------------------------\n",
      "Epoch 74\n",
      "Training Loss: 3131.4922\n",
      "Validation Loss: 1864.9315\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 74\n",
      "------------------------------------------------------------\n",
      "Epoch 75\n",
      "Training Loss: 3132.4504\n",
      "Validation Loss: 1825.9775\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 75\n",
      "------------------------------------------------------------\n",
      "Epoch 76\n",
      "Training Loss: 3089.9001\n",
      "Validation Loss: 1853.6220\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 77\n",
      "Training Loss: 3059.6505\n",
      "Validation Loss: 1812.6021\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 77\n",
      "------------------------------------------------------------\n",
      "Epoch 78\n",
      "Training Loss: 3021.7093\n",
      "Validation Loss: 1850.1931\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 79\n",
      "Training Loss: 2963.7522\n",
      "Validation Loss: 1791.4311\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 79\n",
      "------------------------------------------------------------\n",
      "Epoch 80\n",
      "Training Loss: 2956.2988\n",
      "Validation Loss: 1836.1136\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 81\n",
      "Training Loss: 2924.7465\n",
      "Validation Loss: 1688.0836\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 81\n",
      "------------------------------------------------------------\n",
      "Epoch 82\n",
      "Training Loss: 2882.5356\n",
      "Validation Loss: 1648.5871\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 82\n",
      "------------------------------------------------------------\n",
      "Epoch 83\n",
      "Training Loss: 2822.8022\n",
      "Validation Loss: 1606.1023\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 83\n",
      "------------------------------------------------------------\n",
      "Epoch 84\n",
      "Training Loss: 2873.7159\n",
      "Validation Loss: 1643.6524\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 85\n",
      "Training Loss: 2806.6371\n",
      "Validation Loss: 1925.0175\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 86\n",
      "Training Loss: 2751.0942\n",
      "Validation Loss: 1558.8551\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 86\n",
      "------------------------------------------------------------\n",
      "Epoch 87\n",
      "Training Loss: 2758.8047\n",
      "Validation Loss: 1597.6601\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 88\n",
      "Training Loss: 2720.6569\n",
      "Validation Loss: 1607.1177\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 89\n",
      "Training Loss: 2728.8007\n",
      "Validation Loss: 1517.2746\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 89\n",
      "------------------------------------------------------------\n",
      "Epoch 90\n",
      "Training Loss: 2683.7700\n",
      "Validation Loss: 1529.0556\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 91\n",
      "Training Loss: 2656.3609\n",
      "Validation Loss: 1488.5983\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 91\n",
      "------------------------------------------------------------\n",
      "Epoch 92\n",
      "Training Loss: 2664.3234\n",
      "Validation Loss: 1501.4138\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 93\n",
      "Training Loss: 2657.6677\n",
      "Validation Loss: 1460.8020\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 93\n",
      "------------------------------------------------------------\n",
      "Epoch 94\n",
      "Training Loss: 2612.9452\n",
      "Validation Loss: 1606.2358\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 95\n",
      "Training Loss: 2556.4351\n",
      "Validation Loss: 1534.0123\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 96\n",
      "Training Loss: 2588.5053\n",
      "Validation Loss: 1464.8572\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 97\n",
      "Training Loss: 2481.3763\n",
      "Validation Loss: 1509.4628\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 98\n",
      "Training Loss: 2499.4899\n",
      "Validation Loss: 1437.5485\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 98\n",
      "------------------------------------------------------------\n",
      "Epoch 99\n",
      "Training Loss: 2545.0891\n",
      "Validation Loss: 1405.4897\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 99\n",
      "------------------------------------------------------------\n",
      "Epoch 100\n",
      "Training Loss: 2502.4323\n",
      "Validation Loss: 1394.8855\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 100\n",
      "------------------------------------------------------------\n",
      "Epoch 101\n",
      "Training Loss: 2475.8082\n",
      "Validation Loss: 1369.5622\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 101\n",
      "------------------------------------------------------------\n",
      "Epoch 102\n",
      "Training Loss: 2450.4560\n",
      "Validation Loss: 1414.0234\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 103\n",
      "Training Loss: 2424.8963\n",
      "Validation Loss: 1371.7282\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 104\n",
      "Training Loss: 2430.1605\n",
      "Validation Loss: 1621.1014\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 105\n",
      "Training Loss: 2393.6333\n",
      "Validation Loss: 1343.8855\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 105\n",
      "------------------------------------------------------------\n",
      "Epoch 106\n",
      "Training Loss: 2369.8591\n",
      "Validation Loss: 1426.5533\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 107\n",
      "Training Loss: 2358.1208\n",
      "Validation Loss: 1377.0399\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 108\n",
      "Training Loss: 2397.4360\n",
      "Validation Loss: 1399.0430\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 109\n",
      "Training Loss: 2352.9071\n",
      "Validation Loss: 1682.1393\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 110\n",
      "Training Loss: 2353.2119\n",
      "Validation Loss: 1292.4824\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 110\n",
      "------------------------------------------------------------\n",
      "Epoch 111\n",
      "Training Loss: 2292.7728\n",
      "Validation Loss: 1276.9119\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 111\n",
      "------------------------------------------------------------\n",
      "Epoch 112\n",
      "Training Loss: 2302.7902\n",
      "Validation Loss: 1329.6990\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 113\n",
      "Training Loss: 2297.7747\n",
      "Validation Loss: 1271.3518\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 113\n",
      "------------------------------------------------------------\n",
      "Epoch 114\n",
      "Training Loss: 2272.5857\n",
      "Validation Loss: 1239.9116\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 114\n",
      "------------------------------------------------------------\n",
      "Epoch 115\n",
      "Training Loss: 2247.8668\n",
      "Validation Loss: 1333.1445\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 116\n",
      "Training Loss: 2227.5104\n",
      "Validation Loss: 1347.8532\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 117\n",
      "Training Loss: 2237.1286\n",
      "Validation Loss: 1233.1098\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 117\n",
      "------------------------------------------------------------\n",
      "Epoch 118\n",
      "Training Loss: 2225.7448\n",
      "Validation Loss: 1275.6717\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 119\n",
      "Training Loss: 2158.8580\n",
      "Validation Loss: 1249.3334\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 120\n",
      "Training Loss: 2200.4386\n",
      "Validation Loss: 1250.9336\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 121\n",
      "Training Loss: 2155.7878\n",
      "Validation Loss: 1220.4042\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 121\n",
      "------------------------------------------------------------\n",
      "Epoch 122\n",
      "Training Loss: 2140.6809\n",
      "Validation Loss: 1220.5326\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 123\n",
      "Training Loss: 2175.9201\n",
      "Validation Loss: 1171.0109\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 123\n",
      "------------------------------------------------------------\n",
      "Epoch 124\n",
      "Training Loss: 2104.3679\n",
      "Validation Loss: 1199.0200\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 125\n",
      "Training Loss: 2128.7538\n",
      "Validation Loss: 1254.7104\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 126\n",
      "Training Loss: 2103.2465\n",
      "Validation Loss: 1142.0281\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 126\n",
      "------------------------------------------------------------\n",
      "Epoch 127\n",
      "Training Loss: 2113.9249\n",
      "Validation Loss: 1140.4976\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 127\n",
      "------------------------------------------------------------\n",
      "Epoch 128\n",
      "Training Loss: 2108.4080\n",
      "Validation Loss: 1146.2422\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 129\n",
      "Training Loss: 2054.3563\n",
      "Validation Loss: 1220.1639\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 130\n",
      "Training Loss: 2057.9927\n",
      "Validation Loss: 1129.7898\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 130\n",
      "------------------------------------------------------------\n",
      "Epoch 131\n",
      "Training Loss: 2031.7706\n",
      "Validation Loss: 1110.8086\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 131\n",
      "------------------------------------------------------------\n",
      "Epoch 132\n",
      "Training Loss: 2013.4335\n",
      "Validation Loss: 1098.8882\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 132\n",
      "------------------------------------------------------------\n",
      "Epoch 133\n",
      "Training Loss: 1993.8227\n",
      "Validation Loss: 1167.1091\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 134\n",
      "Training Loss: 2005.8787\n",
      "Validation Loss: 1096.0057\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 134\n",
      "------------------------------------------------------------\n",
      "Epoch 135\n",
      "Training Loss: 2004.5843\n",
      "Validation Loss: 1110.3851\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 136\n",
      "Training Loss: 1987.0956\n",
      "Validation Loss: 1085.6022\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 136\n",
      "------------------------------------------------------------\n",
      "Epoch 137\n",
      "Training Loss: 1969.4653\n",
      "Validation Loss: 1105.9619\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 138\n",
      "Training Loss: 1974.9908\n",
      "Validation Loss: 1106.4413\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 139\n",
      "Training Loss: 1957.9705\n",
      "Validation Loss: 1137.6132\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 140\n",
      "Training Loss: 1933.1588\n",
      "Validation Loss: 1091.2392\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 141\n",
      "Training Loss: 1917.0881\n",
      "Validation Loss: 1046.7147\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 141\n",
      "------------------------------------------------------------\n",
      "Epoch 142\n",
      "Training Loss: 1936.2587\n",
      "Validation Loss: 1038.9040\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 142\n",
      "------------------------------------------------------------\n",
      "Epoch 143\n",
      "Training Loss: 1914.0500\n",
      "Validation Loss: 1038.6042\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 143\n",
      "------------------------------------------------------------\n",
      "Epoch 144\n",
      "Training Loss: 1919.9375\n",
      "Validation Loss: 1075.9587\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 145\n",
      "Training Loss: 1895.2664\n",
      "Validation Loss: 1058.0936\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 146\n",
      "Training Loss: 1886.5889\n",
      "Validation Loss: 1042.0518\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 147\n",
      "Training Loss: 1895.4349\n",
      "Validation Loss: 1030.3173\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 147\n",
      "------------------------------------------------------------\n",
      "Epoch 148\n",
      "Training Loss: 1868.2987\n",
      "Validation Loss: 1075.9559\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 149\n",
      "Training Loss: 1849.9505\n",
      "Validation Loss: 1032.4046\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 150\n",
      "Training Loss: 1857.3048\n",
      "Validation Loss: 1096.0009\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 151\n",
      "Training Loss: 1819.9915\n",
      "Validation Loss: 1023.5632\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 151\n",
      "------------------------------------------------------------\n",
      "Epoch 152\n",
      "Training Loss: 1842.6258\n",
      "Validation Loss: 1059.9457\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 153\n",
      "Training Loss: 1844.4225\n",
      "Validation Loss: 998.5721\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 153\n",
      "------------------------------------------------------------\n",
      "Epoch 154\n",
      "Training Loss: 1826.7977\n",
      "Validation Loss: 980.0427\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 154\n",
      "------------------------------------------------------------\n",
      "Epoch 155\n",
      "Training Loss: 1826.8908\n",
      "Validation Loss: 998.3029\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 156\n",
      "Training Loss: 1796.5526\n",
      "Validation Loss: 1025.6568\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 157\n",
      "Training Loss: 1805.0547\n",
      "Validation Loss: 1019.1766\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 158\n",
      "Training Loss: 1767.5516\n",
      "Validation Loss: 1030.6099\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 159\n",
      "Training Loss: 1760.4403\n",
      "Validation Loss: 949.5255\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 159\n",
      "------------------------------------------------------------\n",
      "Epoch 160\n",
      "Training Loss: 1797.6564\n",
      "Validation Loss: 1020.0752\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 161\n",
      "Training Loss: 1788.0369\n",
      "Validation Loss: 973.9128\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 162\n",
      "Training Loss: 1766.9952\n",
      "Validation Loss: 962.8551\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 163\n",
      "Training Loss: 1764.6938\n",
      "Validation Loss: 1000.7810\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 164\n",
      "Training Loss: 1741.3763\n",
      "Validation Loss: 985.6235\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 165\n",
      "Training Loss: 1723.6958\n",
      "Validation Loss: 1002.8844\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 166\n",
      "Training Loss: 1592.6740\n",
      "Validation Loss: 890.5244\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 166\n",
      "------------------------------------------------------------\n",
      "Epoch 167\n",
      "Training Loss: 1547.5995\n",
      "Validation Loss: 877.3772\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 167\n",
      "------------------------------------------------------------\n",
      "Epoch 168\n",
      "Training Loss: 1554.5449\n",
      "Validation Loss: 856.1753\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 168\n",
      "------------------------------------------------------------\n",
      "Epoch 169\n",
      "Training Loss: 1515.6306\n",
      "Validation Loss: 885.0625\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 170\n",
      "Training Loss: 1534.7666\n",
      "Validation Loss: 875.4308\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 171\n",
      "Training Loss: 1504.4974\n",
      "Validation Loss: 857.2748\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 172\n",
      "Training Loss: 1519.6883\n",
      "Validation Loss: 842.7099\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 172\n",
      "------------------------------------------------------------\n",
      "Epoch 173\n",
      "Training Loss: 1500.7999\n",
      "Validation Loss: 841.9704\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 173\n",
      "------------------------------------------------------------\n",
      "Epoch 174\n",
      "Training Loss: 1503.4280\n",
      "Validation Loss: 840.0390\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 174\n",
      "------------------------------------------------------------\n",
      "Epoch 175\n",
      "Training Loss: 1495.9863\n",
      "Validation Loss: 846.9986\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 176\n",
      "Training Loss: 1482.9216\n",
      "Validation Loss: 834.4206\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 176\n",
      "------------------------------------------------------------\n",
      "Epoch 177\n",
      "Training Loss: 1473.4723\n",
      "Validation Loss: 832.0709\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 177\n",
      "------------------------------------------------------------\n",
      "Epoch 178\n",
      "Training Loss: 1476.1314\n",
      "Validation Loss: 823.8550\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 178\n",
      "------------------------------------------------------------\n",
      "Epoch 179\n",
      "Training Loss: 1467.8111\n",
      "Validation Loss: 826.4914\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 180\n",
      "Training Loss: 1476.4980\n",
      "Validation Loss: 835.4512\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 181\n",
      "Training Loss: 1460.3950\n",
      "Validation Loss: 812.5135\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 181\n",
      "------------------------------------------------------------\n",
      "Epoch 182\n",
      "Training Loss: 1475.0759\n",
      "Validation Loss: 844.5474\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 183\n",
      "Training Loss: 1455.5949\n",
      "Validation Loss: 806.9552\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 183\n",
      "------------------------------------------------------------\n",
      "Epoch 184\n",
      "Training Loss: 1440.4258\n",
      "Validation Loss: 808.1735\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 185\n",
      "Training Loss: 1451.1412\n",
      "Validation Loss: 810.8815\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 186\n",
      "Training Loss: 1450.6517\n",
      "Validation Loss: 800.4439\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 186\n",
      "------------------------------------------------------------\n",
      "Epoch 187\n",
      "Training Loss: 1434.6489\n",
      "Validation Loss: 824.4644\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 188\n",
      "Training Loss: 1446.4107\n",
      "Validation Loss: 816.7205\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 189\n",
      "Training Loss: 1426.6608\n",
      "Validation Loss: 804.3880\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 190\n",
      "Training Loss: 1423.4877\n",
      "Validation Loss: 793.6445\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 190\n",
      "------------------------------------------------------------\n",
      "Epoch 191\n",
      "Training Loss: 1419.6375\n",
      "Validation Loss: 803.3629\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 192\n",
      "Training Loss: 1409.2156\n",
      "Validation Loss: 782.7007\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 192\n",
      "------------------------------------------------------------\n",
      "Epoch 193\n",
      "Training Loss: 1418.4027\n",
      "Validation Loss: 788.6700\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 194\n",
      "Training Loss: 1407.6211\n",
      "Validation Loss: 849.8014\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 195\n",
      "Training Loss: 1388.3404\n",
      "Validation Loss: 787.0587\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 196\n",
      "Training Loss: 1410.5796\n",
      "Validation Loss: 788.6484\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 197\n",
      "Training Loss: 1401.6199\n",
      "Validation Loss: 782.6914\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 197\n",
      "------------------------------------------------------------\n",
      "Epoch 198\n",
      "Training Loss: 1401.0854\n",
      "Validation Loss: 774.2048\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 198\n",
      "------------------------------------------------------------\n",
      "Epoch 199\n",
      "Training Loss: 1383.2920\n",
      "Validation Loss: 775.0600\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 200\n",
      "Training Loss: 1375.9345\n",
      "Validation Loss: 787.3808\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 201\n",
      "Training Loss: 1389.8051\n",
      "Validation Loss: 778.8412\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 202\n",
      "Training Loss: 1381.2249\n",
      "Validation Loss: 774.3507\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 203\n",
      "Training Loss: 1394.2876\n",
      "Validation Loss: 769.9784\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 203\n",
      "------------------------------------------------------------\n",
      "Epoch 204\n",
      "Training Loss: 1371.5760\n",
      "Validation Loss: 758.1343\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 204\n",
      "------------------------------------------------------------\n",
      "Epoch 205\n",
      "Training Loss: 1377.4463\n",
      "Validation Loss: 759.5595\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 206\n",
      "Training Loss: 1372.2193\n",
      "Validation Loss: 771.6659\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 207\n",
      "Training Loss: 1380.2482\n",
      "Validation Loss: 761.5541\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 208\n",
      "Training Loss: 1370.0758\n",
      "Validation Loss: 778.4107\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 209\n",
      "Training Loss: 1359.5666\n",
      "Validation Loss: 756.1554\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 209\n",
      "------------------------------------------------------------\n",
      "Epoch 210\n",
      "Training Loss: 1349.4454\n",
      "Validation Loss: 757.5929\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 211\n",
      "Training Loss: 1366.2487\n",
      "Validation Loss: 738.9489\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 211\n",
      "------------------------------------------------------------\n",
      "Epoch 212\n",
      "Training Loss: 1354.5898\n",
      "Validation Loss: 751.6958\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 213\n",
      "Training Loss: 1339.7847\n",
      "Validation Loss: 759.3324\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 214\n",
      "Training Loss: 1332.8556\n",
      "Validation Loss: 768.2530\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 215\n",
      "Training Loss: 1334.9423\n",
      "Validation Loss: 750.7870\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 216\n",
      "Training Loss: 1335.9668\n",
      "Validation Loss: 746.8226\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 217\n",
      "Training Loss: 1345.7356\n",
      "Validation Loss: 744.5248\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 218\n",
      "Training Loss: 1268.4980\n",
      "Validation Loss: 724.8021\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 218\n",
      "------------------------------------------------------------\n",
      "Epoch 219\n",
      "Training Loss: 1261.9874\n",
      "Validation Loss: 725.5104\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 220\n",
      "Training Loss: 1250.7472\n",
      "Validation Loss: 727.7994\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 221\n",
      "Training Loss: 1257.3039\n",
      "Validation Loss: 708.1462\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 221\n",
      "------------------------------------------------------------\n",
      "Epoch 222\n",
      "Training Loss: 1247.2467\n",
      "Validation Loss: 709.1056\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 223\n",
      "Training Loss: 1249.2628\n",
      "Validation Loss: 714.4786\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 224\n",
      "Training Loss: 1255.1141\n",
      "Validation Loss: 715.3129\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 225\n",
      "Training Loss: 1264.9671\n",
      "Validation Loss: 702.5412\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 225\n",
      "------------------------------------------------------------\n",
      "Epoch 226\n",
      "Training Loss: 1231.1185\n",
      "Validation Loss: 692.0838\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 226\n",
      "------------------------------------------------------------\n",
      "Epoch 227\n",
      "Training Loss: 1243.6885\n",
      "Validation Loss: 704.6577\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 228\n",
      "Training Loss: 1239.2696\n",
      "Validation Loss: 703.7586\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 229\n",
      "Training Loss: 1238.2617\n",
      "Validation Loss: 695.1336\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 230\n",
      "Training Loss: 1233.6746\n",
      "Validation Loss: 700.8393\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 231\n",
      "Training Loss: 1240.8909\n",
      "Validation Loss: 694.2414\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 232\n",
      "Training Loss: 1229.2718\n",
      "Validation Loss: 691.7169\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 232\n",
      "------------------------------------------------------------\n",
      "Epoch 233\n",
      "Training Loss: 1218.9072\n",
      "Validation Loss: 698.0707\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 234\n",
      "Training Loss: 1226.6578\n",
      "Validation Loss: 698.4353\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 235\n",
      "Training Loss: 1204.7690\n",
      "Validation Loss: 689.7509\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 235\n",
      "------------------------------------------------------------\n",
      "Epoch 236\n",
      "Training Loss: 1221.6604\n",
      "Validation Loss: 701.0611\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 237\n",
      "Training Loss: 1221.1980\n",
      "Validation Loss: 690.1032\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 238\n",
      "Training Loss: 1206.0539\n",
      "Validation Loss: 696.2277\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 239\n",
      "Training Loss: 1205.3926\n",
      "Validation Loss: 685.7566\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 239\n",
      "------------------------------------------------------------\n",
      "Epoch 240\n",
      "Training Loss: 1210.6683\n",
      "Validation Loss: 685.9866\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 241\n",
      "Training Loss: 1207.4273\n",
      "Validation Loss: 697.7794\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 242\n",
      "Training Loss: 1200.4835\n",
      "Validation Loss: 707.4920\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 243\n",
      "Training Loss: 1200.0864\n",
      "Validation Loss: 686.2857\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 244\n",
      "Training Loss: 1209.2724\n",
      "Validation Loss: 687.8229\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 245\n",
      "Training Loss: 1202.9458\n",
      "Validation Loss: 685.2503\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 245\n",
      "------------------------------------------------------------\n",
      "Epoch 246\n",
      "Training Loss: 1219.9104\n",
      "Validation Loss: 688.4800\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 247\n",
      "Training Loss: 1200.0476\n",
      "Validation Loss: 683.5543\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 247\n",
      "------------------------------------------------------------\n",
      "Epoch 248\n",
      "Training Loss: 1212.1778\n",
      "Validation Loss: 687.5930\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 249\n",
      "Training Loss: 1206.3008\n",
      "Validation Loss: 678.5706\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 249\n",
      "------------------------------------------------------------\n",
      "Epoch 250\n",
      "Training Loss: 1188.4667\n",
      "Validation Loss: 673.8548\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 250\n",
      "------------------------------------------------------------\n",
      "Epoch 251\n",
      "Training Loss: 1176.9917\n",
      "Validation Loss: 676.1453\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 252\n",
      "Training Loss: 1196.8604\n",
      "Validation Loss: 677.3057\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 253\n",
      "Training Loss: 1186.0491\n",
      "Validation Loss: 672.5757\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 253\n",
      "------------------------------------------------------------\n",
      "Epoch 254\n",
      "Training Loss: 1196.9515\n",
      "Validation Loss: 667.2703\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 254\n",
      "------------------------------------------------------------\n",
      "Epoch 255\n",
      "Training Loss: 1190.8579\n",
      "Validation Loss: 679.5505\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 256\n",
      "Training Loss: 1190.7210\n",
      "Validation Loss: 663.8485\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 256\n",
      "------------------------------------------------------------\n",
      "Epoch 257\n",
      "Training Loss: 1178.0968\n",
      "Validation Loss: 666.7140\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 258\n",
      "Training Loss: 1171.2093\n",
      "Validation Loss: 661.4787\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 258\n",
      "------------------------------------------------------------\n",
      "Epoch 259\n",
      "Training Loss: 1175.6796\n",
      "Validation Loss: 668.8379\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 260\n",
      "Training Loss: 1171.2695\n",
      "Validation Loss: 670.5523\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 261\n",
      "Training Loss: 1165.0064\n",
      "Validation Loss: 668.3259\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 262\n",
      "Training Loss: 1180.0454\n",
      "Validation Loss: 667.9872\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 263\n",
      "Training Loss: 1184.4342\n",
      "Validation Loss: 669.4493\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 264\n",
      "Training Loss: 1164.8480\n",
      "Validation Loss: 663.2306\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 265\n",
      "Training Loss: 1153.5023\n",
      "Validation Loss: 654.8578\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 265\n",
      "------------------------------------------------------------\n",
      "Epoch 266\n",
      "Training Loss: 1138.4409\n",
      "Validation Loss: 655.6886\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 267\n",
      "Training Loss: 1145.6540\n",
      "Validation Loss: 648.6628\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 267\n",
      "------------------------------------------------------------\n",
      "Epoch 268\n",
      "Training Loss: 1139.3443\n",
      "Validation Loss: 657.6306\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 269\n",
      "Training Loss: 1110.5195\n",
      "Validation Loss: 648.5870\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 269\n",
      "------------------------------------------------------------\n",
      "Epoch 270\n",
      "Training Loss: 1135.2352\n",
      "Validation Loss: 649.4456\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 271\n",
      "Training Loss: 1135.6688\n",
      "Validation Loss: 646.8395\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 271\n",
      "------------------------------------------------------------\n",
      "Epoch 272\n",
      "Training Loss: 1143.5118\n",
      "Validation Loss: 646.2352\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 272\n",
      "------------------------------------------------------------\n",
      "Epoch 273\n",
      "Training Loss: 1132.0033\n",
      "Validation Loss: 642.0385\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 273\n",
      "------------------------------------------------------------\n",
      "Epoch 274\n",
      "Training Loss: 1142.3661\n",
      "Validation Loss: 646.9301\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 275\n",
      "Training Loss: 1127.4162\n",
      "Validation Loss: 642.9970\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 276\n",
      "Training Loss: 1128.0152\n",
      "Validation Loss: 643.5165\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 277\n",
      "Training Loss: 1123.6661\n",
      "Validation Loss: 641.3815\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 277\n",
      "------------------------------------------------------------\n",
      "Epoch 278\n",
      "Training Loss: 1136.2834\n",
      "Validation Loss: 643.9061\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 279\n",
      "Training Loss: 1114.1179\n",
      "Validation Loss: 641.6217\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 280\n",
      "Training Loss: 1118.8297\n",
      "Validation Loss: 640.4339\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 280\n",
      "------------------------------------------------------------\n",
      "Epoch 281\n",
      "Training Loss: 1122.5825\n",
      "Validation Loss: 638.4713\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 281\n",
      "------------------------------------------------------------\n",
      "Epoch 282\n",
      "Training Loss: 1114.1125\n",
      "Validation Loss: 641.1334\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 283\n",
      "Training Loss: 1129.0246\n",
      "Validation Loss: 638.7485\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 284\n",
      "Training Loss: 1111.9120\n",
      "Validation Loss: 637.6179\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 284\n",
      "------------------------------------------------------------\n",
      "Epoch 285\n",
      "Training Loss: 1113.5758\n",
      "Validation Loss: 635.8706\n",
      "Learning Rate: 0.000013\n",
      "Model saved at epoch 285\n",
      "------------------------------------------------------------\n",
      "Epoch 286\n",
      "Training Loss: 1123.0294\n",
      "Validation Loss: 638.1945\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 287\n",
      "Training Loss: 1113.8093\n",
      "Validation Loss: 638.6123\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 288\n",
      "Training Loss: 1100.2711\n",
      "Validation Loss: 636.5098\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 289\n",
      "Training Loss: 1119.0669\n",
      "Validation Loss: 637.9368\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 290\n",
      "Training Loss: 1119.9057\n",
      "Validation Loss: 640.3453\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 291\n",
      "Training Loss: 1110.6898\n",
      "Validation Loss: 637.1321\n",
      "Learning Rate: 0.000006\n",
      "------------------------------------------------------------\n",
      "Epoch 292\n",
      "Training Loss: 1106.7092\n",
      "Validation Loss: 634.7546\n",
      "Learning Rate: 0.000006\n",
      "Model saved at epoch 292\n",
      "------------------------------------------------------------\n",
      "Epoch 293\n",
      "Training Loss: 1093.6610\n",
      "Validation Loss: 635.1487\n",
      "Learning Rate: 0.000006\n",
      "------------------------------------------------------------\n",
      "Epoch 294\n",
      "Training Loss: 1101.7040\n",
      "Validation Loss: 634.4901\n",
      "Learning Rate: 0.000006\n",
      "Model saved at epoch 294\n",
      "------------------------------------------------------------\n",
      "Epoch 295\n",
      "Training Loss: 1098.4589\n",
      "Validation Loss: 631.8070\n",
      "Learning Rate: 0.000006\n",
      "Model saved at epoch 295\n",
      "------------------------------------------------------------\n",
      "Epoch 296\n",
      "Training Loss: 1099.4058\n",
      "Validation Loss: 629.5471\n",
      "Learning Rate: 0.000006\n",
      "Model saved at epoch 296\n",
      "------------------------------------------------------------\n",
      "Epoch 297\n",
      "Training Loss: 1080.7578\n",
      "Validation Loss: 626.7052\n",
      "Learning Rate: 0.000006\n",
      "Model saved at epoch 297\n",
      "------------------------------------------------------------\n",
      "Epoch 298\n",
      "Training Loss: 1089.6603\n",
      "Validation Loss: 629.0474\n",
      "Learning Rate: 0.000006\n",
      "------------------------------------------------------------\n",
      "Epoch 299\n",
      "Training Loss: 1104.0010\n",
      "Validation Loss: 631.0119\n",
      "Learning Rate: 0.000006\n",
      "------------------------------------------------------------\n",
      "Epoch 300\n",
      "Training Loss: 1096.1636\n",
      "Validation Loss: 629.7442\n",
      "Learning Rate: 0.000006\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # データの読み込み\n",
    "    data_pairs = [\n",
    "        #\n",
    "        # 第三回収集データ\n",
    "        #\n",
    "        # # 立ちっぱなし\n",
    "         ('./data/20241115test3/Opti-track/Take 2024-11-15 03.20.00 PM.csv',\n",
    "          './data/20241115test3/insoleSensor/20241115_152500_left.csv',\n",
    "          './data/20241115test3/insoleSensor/20241115_152500_right.csv'),\n",
    "        # お辞儀\n",
    "        ('./data/20241115test3/Opti-track/Take 2024-11-15 03.26.00 PM.csv',\n",
    "         './data/20241115test3/insoleSensor/20241115_153100_left.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_153100_right.csv'),\n",
    "        # 体の横の傾け\n",
    "        ('./data/20241115test3/Opti-track/Take 2024-11-15 03.32.00 PM.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_153700_left.csv', \n",
    "        './data/20241115test3/insoleSensor/20241115_153700_right.csv'),\n",
    "        # 立つ座る\n",
    "        ('./data/20241115test3/Opti-track/Take 2024-11-15 03.38.00 PM.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_154300_left.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_154300_right.csv'),\n",
    "        # スクワット\n",
    "        ('./data/20241115test3/Opti-track/Take 2024-11-15 03.44.00 PM.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_154900_left.csv', \n",
    "         './data/20241115test3/insoleSensor/20241115_154900_right.csv'),\n",
    "         # 総合(test3)\n",
    "        #('./data/20241115test3/Opti-track/Take 2024-11-15 03.50.00 PM.csv', \n",
    "        # './data/20241115test3/insoleSensor/20241115_155500_left.csv', \n",
    "        # './data/20241115test3/insoleSensor/20241115_155500_right.csv'),\n",
    "\n",
    "        # 釘宮くん\n",
    "        ('./data/20241212test4/Opti-track/Take 2024-12-12 03.06.59 PM.csv',\n",
    "         './data/20241212test4/insoleSensor/20241212_152700_left.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_152700_right.csv'),\n",
    "        # 百田くん\n",
    "        ('./data/20241212test4/Opti-track/Take 2024-12-12 03.45.00 PM.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_160501_left.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_160501_right.csv'),\n",
    "        # # # # 渡辺(me)\n",
    "         ('./data/20241212test4/Opti-track/Take 2024-12-12 04.28.00 PM.csv', \n",
    "          './data/20241212test4/insoleSensor/20241212_164800_left.csv', \n",
    "          './data/20241212test4/insoleSensor/20241212_164800_right.csv'),\n",
    "        # にるぱむさん\n",
    "        ('./data/20241212test4/Opti-track/Take 2024-12-12 05.17.59 PM.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_173800_left.csv', \n",
    "         './data/20241212test4/insoleSensor/20241212_173800_right.csv')\n",
    "    ]\n",
    "    \n",
    "    # データの読み込みと結合\n",
    "    seq_length=3\n",
    "    window_size=1\n",
    "    skeleton_data, pressure_data_left, pressure_data_right, decoder_input, skeleton_label = load_and_combine_data(data_pairs,seq_length,window_size)\n",
    "    \n",
    "    # numpy配列に変換\n",
    "    skeleton_data = skeleton_data.to_numpy()\n",
    "    decoder_input = decoder_input\n",
    "\n",
    "    # 圧力、回転、加速度データの前処理\n",
    "    input_features, sensor_scalers = preprocess_pressure_data(\n",
    "        pressure_data_left,\n",
    "        pressure_data_right\n",
    "    )\n",
    "    print(input_features.shape)\n",
    "    \n",
    "    # データの分割\n",
    "    train_input, val_input, train_skeleton, val_skeleton, train_decoder_input, val_decoder_input = train_test_split(\n",
    "        input_features, \n",
    "        skeleton_label,\n",
    "        decoder_input,\n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    print(train_decoder_input.shape)\n",
    "    print(val_decoder_input.shape)\n",
    "    # デバイスの設定\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # モデルのパラメータ設定\n",
    "    input_dim = input_features.shape[1]  # 圧力+回転+加速度の合計次元数\n",
    "    d_model = 512\n",
    "    nhead = 8\n",
    "    num_encoder_layers = 6\n",
    "    num_joints = skeleton_data.shape[1] // 3  # 3D座標なので3で割る\n",
    "    dropout = 0.1\n",
    "    batch_size = 128\n",
    "\n",
    "    # データローダーの設定\n",
    "    train_dataset = PressureSkeletonDataset(train_input, train_skeleton, train_decoder_input)\n",
    "    val_dataset = PressureSkeletonDataset(val_input, val_skeleton, val_decoder_input)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(\"Checking final training and validation data...\")\n",
    "    print(\"Train input NaN count:\", np.isnan(train_input).sum(), \"Inf count:\", np.isinf(train_input).sum())\n",
    "    print(\"Train skeleton NaN count:\", np.isnan(train_skeleton).sum(), \"Inf count:\", np.isinf(train_skeleton).sum())\n",
    "    \n",
    "    # モデルの初期化\n",
    "    model = EnhancedSkeletonTransformer(\n",
    "        input_dim= input_features.shape[1], # input_dim,\n",
    "        d_model= d_model,\n",
    "        nhead= nhead,\n",
    "        num_encoder_layers= num_encoder_layers,\n",
    "        num_joints=num_joints,\n",
    "        num_dims=3,\n",
    "        dropout=dropout,\n",
    "        seq_length=seq_length,\n",
    "        window_size=window_size\n",
    "    ).to(device)\n",
    "\n",
    "    # 損失関数、オプティマイザ、スケジューラの設定\n",
    "    # criterion = torch.nn.MSELoss()  # 必要に応じてカスタム損失関数に変更可能\n",
    "    criterion = EnhancedSkeletonLoss_WithAngleConstrains(alpha=1.0,gamma=0.5,window_size=window_size)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        weight_decay=0.001,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    # トレーニング実行\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        num_epochs=300,\n",
    "        save_path='./weight/best_skeleton_model.pth',\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # モデルの保存\n",
    "    final_checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'sensor_scalers': sensor_scalers,\n",
    "        # 'skeleton_skaler': skeleton_scaler,\n",
    "        'model_config': {\n",
    "            'input_dim': input_dim,\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_encoder_layers': num_encoder_layers,\n",
    "            'num_joints': num_joints\n",
    "        }\n",
    "    }\n",
    "    torch.save(final_checkpoint, './weight/final_skeleton_model.pth')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15d75682-2769-4ba6-aef4-061fed518852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip freeze > requirements.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56a252-3fe0-4fe8-9633-3a591c44bf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f2617-0947-4329-9b42-b30da55f7ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdb9a7-c658-4d61-83d7-abed0fb4e954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bc1cd-55f8-4ede-941c-8047a6024962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4daf562-d2d9-4dad-8fd2-32b2e4bec770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9320ef-4255-407a-a160-95d9689fd34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24c0f76e-b4e9-417a-b49c-b1f20d0b24e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3961, 0.2403, 0.1457, 0.0884, 0.0536, 0.0325, 0.0197, 0.0120, 0.0073,\n",
      "        0.0044])\n"
     ]
    }
   ],
   "source": [
    "def compute_exponential_weights(k, m):\n",
    "    \"\"\"计算指数衰减权重 w_i = exp(-m * i)，并归一化\"\"\"\n",
    "    indices = torch.arange(k)  # 生成 i = 0, 1, ..., k-1\n",
    "    weights = torch.exp(-m * indices)  # 计算 w_i\n",
    "    return weights / weights.sum()  # 归一化，使得所有权重之和为 1\n",
    "\n",
    "# 示例：计算最近 5 个时间步的权重\n",
    "k = 10  # 观察的时间窗口\n",
    "m = 0.5  # 衰减因子（越大表示衰减越快）\n",
    "\n",
    "weights = compute_exponential_weights(k, m)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8749eabc-f3f6-4372-a1b5-1f8e11a89ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSkeletonLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # MSE損失\n",
    "        mse_loss = F.mse_loss(pred, target)\n",
    "        batch_size = int(pred.shape[0])\n",
    "        # 変化量の損失\n",
    "        motion_loss = F.mse_loss(\n",
    "            pred[1:] - pred[:-1],\n",
    "            target[1:] - target[:-1]\n",
    "        )\n",
    "        return self.alpha * mse_loss/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0cb7fa6-331d-4708-bec2-79b414891c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction process...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "torch.Size([3004, 1, 63])\n",
      "Making predictions...\n",
      "Prediction shape: torch.Size([2994, 63])\n",
      "Loss: 2.3911569118499756\n",
      "Loss: 31354.205078125\n",
      "\n",
      "Saving predictions...\n",
      "Predictions saved to ./output/predicted_skeleton.csv\n",
      "[[ 5.34281349e+00  8.76214050e+02  1.26494622e+00 ... -8.76946793e+01\n",
      "   8.83348846e+01 -1.10904175e+02]\n",
      " [ 5.27827883e+00  8.71354065e+02  1.48741972e+00 ... -7.71835480e+01\n",
      "   7.92430649e+01 -1.09169250e+02]\n",
      " [ 4.68315840e+00  8.74559570e+02  1.57126701e+00 ... -7.00811081e+01\n",
      "   8.24047928e+01 -1.08046791e+02]\n",
      " ...\n",
      " [ 7.22077084e+00  8.77622681e+02 -5.20166993e-01 ... -9.78867722e+01\n",
      "   9.30679626e+01 -1.64373001e+02]\n",
      " [ 7.26015949e+00  8.77730652e+02 -3.90151173e-01 ... -1.00685295e+02\n",
      "   9.30938797e+01 -1.66000519e+02]\n",
      " [ 7.35559511e+00  8.77811401e+02 -2.89777517e-01 ... -1.02342613e+02\n",
      "   9.31589127e+01 -1.66916229e+02]]\n",
      "Prediction process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_pressure_data(left_data, right_data):\n",
    "    \"\"\"圧力、回転、加速度データの前処理\"\"\"\n",
    "    # 左足データから各種センサー値を抽出\n",
    "    left_pressure = left_data.iloc[:, :35]\n",
    "    left_rotation = left_data.iloc[:, 35:38]\n",
    "    left_accel = left_data.iloc[:, 38:41]\n",
    "\n",
    "    # 右足データから各種センサー値を抽出\n",
    "    right_pressure = right_data.iloc[:, :35]\n",
    "    right_rotation = right_data.iloc[:, 35:38]\n",
    "    right_accel = right_data.iloc[:, 38:41]\n",
    "\n",
    "    # データの結合\n",
    "    pressure_combined = pd.concat([left_pressure, right_pressure], axis=1)\n",
    "    rotation_combined = pd.concat([left_rotation, right_rotation], axis=1)\n",
    "    accel_combined = pd.concat([left_accel, right_accel], axis=1)\n",
    "\n",
    "    # NaN値を補正\n",
    "    pressure_combined = pressure_combined.ffill().bfill()\n",
    "    rotation_combined = rotation_combined.ffill().bfill()\n",
    "    accel_combined = accel_combined.ffill().bfill()\n",
    "\n",
    "    # 移動平均フィルタの適用\n",
    "    window_size = 3\n",
    "    pressure_combined = pressure_combined.rolling(window=window_size, center=True).mean()\n",
    "    rotation_combined = rotation_combined.rolling(window=window_size, center=True).mean()\n",
    "    accel_combined = accel_combined.rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    # NaN値を補間\n",
    "    pressure_combined = pressure_combined.ffill().bfill()\n",
    "    rotation_combined = rotation_combined.ffill().bfill()\n",
    "    accel_combined = accel_combined.ffill().bfill()\n",
    "\n",
    "    # 正規化と標準化\n",
    "    pressure_normalizer = MinMaxScaler()\n",
    "    rotation_normalizer = MinMaxScaler()\n",
    "    accel_normalizer = MinMaxScaler()\n",
    "\n",
    "    pressure_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    rotation_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    accel_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    # データの正規化と標準化\n",
    "    pressure_processed = pressure_standardizer.fit_transform(\n",
    "        pressure_normalizer.fit_transform(pressure_combined)\n",
    "    )\n",
    "    rotation_processed = rotation_standardizer.fit_transform(\n",
    "        rotation_normalizer.fit_transform(rotation_combined)\n",
    "    )\n",
    "    accel_processed = accel_standardizer.fit_transform(\n",
    "        accel_normalizer.fit_transform(accel_combined)\n",
    "    )\n",
    "\n",
    "    # 1次微分と2次微分の計算\n",
    "    pressure_grad1 = np.gradient(pressure_processed, axis=0)\n",
    "    pressure_grad2 = np.gradient(pressure_grad1, axis=0)\n",
    "    \n",
    "    rotation_grad1 = np.gradient(rotation_processed, axis=0)\n",
    "    rotation_grad2 = np.gradient(rotation_grad1, axis=0)\n",
    "    \n",
    "    accel_grad1 = np.gradient(accel_processed, axis=0)\n",
    "    accel_grad2 = np.gradient(accel_grad1, axis=0)\n",
    "\n",
    "    # すべての特徴量を結合（246次元になるはず）\n",
    "    input_features = np.concatenate([\n",
    "        pressure_processed,  # 原特徴量\n",
    "        #pressure_grad1,     # 1次微分\n",
    "        #pressure_grad2,     # 2次微分\n",
    "        rotation_processed,\n",
    "        #rotation_grad1,\n",
    "        #rotation_grad2,\n",
    "        accel_processed,\n",
    "        #accel_grad1,\n",
    "        #accel_grad2\n",
    "    ], axis=1)\n",
    "\n",
    "    return input_features\n",
    "\n",
    "def load_and_preprocess_data(file_pairs):\n",
    "    predictions_all = []\n",
    "    \n",
    "    for skeleton_file, left_file, right_file in file_pairs:\n",
    "        skeleton_data = pd.read_csv(skeleton_file)\n",
    "        pressure_data_left = pd.read_csv(left_file)\n",
    "        pressure_data_right = pd.read_csv(right_file)\n",
    "        \n",
    "        input_features = preprocess_pressure_data(pressure_data_left, pressure_data_right)\n",
    "        min_length = min(len(skeleton_data), len(input_features))\n",
    "        \n",
    "        input_features = input_features.iloc[:min_length]\n",
    "        skeleton_data = skeleton_data.iloc[:min_length]\n",
    "        \n",
    "        predictions_all.append((input_features, skeleton_data))\n",
    "    \n",
    "    return predictions_all\n",
    "\n",
    "def predict_skeleton():\n",
    "    try:\n",
    "        # データの読み込みと前処理\n",
    "        skeleton_data = pd.read_csv('./data/20241115test3/Opti-track/Take 2024-11-15 03.50.00 PM.csv')\n",
    "        pressure_data_left = pd.read_csv('./data/20241115test3/insoleSensor/20241115_155500_left.csv', skiprows=1)\n",
    "        pressure_data_right = pd.read_csv('./data/20241115test3/insoleSensor/20241115_155500_right.csv', skiprows=1)\n",
    "\n",
    "        # 入力データの前処理\n",
    "        input_features = preprocess_pressure_data(pressure_data_left, pressure_data_right)\n",
    "        min_length=min(input_features.shape[0],skeleton_data.shape[0])\n",
    " \n",
    "        # 入力の次元数を取得\n",
    "        seq_length = 3\n",
    "        window_size = 1\n",
    "        m=1\n",
    "        input_dim = input_features.shape[1]\n",
    "        num_joints = skeleton_data.shape[1] // 3\n",
    "\n",
    "        # デバイスの設定\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        skeleton_data=torch.FloatTensor(np.array(skeleton_data)[:min_length]).to(device)\n",
    "\n",
    "        # モデルの初期化（固定パラメータを使用）\n",
    "        model = EnhancedSkeletonTransformer(\n",
    "            input_dim=input_dim,\n",
    "            d_model=512,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=6,\n",
    "            num_joints=num_joints,\n",
    "            num_dims=3,\n",
    "            dropout=0.1,\n",
    "            seq_length=seq_length,\n",
    "            window_size=window_size\n",
    "        ).to(device)\n",
    "\n",
    "        # チェックポイントの読み込み（weights_only=Trueを追加）\n",
    "        checkpoint = torch.load('./weight/best_skeleton_model.pth', map_location=device, weights_only=True)\n",
    "        \n",
    "        # モデルの重みを読み込み\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "\n",
    "        action=torch.zeros((min_length+10,window_size,63)).to(device)\n",
    "        num=np.zeros(min_length+10)\n",
    "        print(action.shape)\n",
    "        # 予測の実行\n",
    "        print(\"Making predictions...\")\n",
    "        predictions=torch.zeros(min_length,63).to(device)\n",
    "        with torch.no_grad():\n",
    "            skeleton_last=torch.zeros((seq_length,63))\n",
    "            skeleton_last=skeleton_last.unsqueeze(0).to(device)\n",
    "            for i in range(min_length):\n",
    "                input_tensor = torch.FloatTensor(input_features)[i].to(device)\n",
    "                input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "                \n",
    "                skeleton_last_pos=add_positional_encoding(skeleton_last)\n",
    "                skeleton_predict_seq=model(input_tensor,skeleton_last)\n",
    "                skeleton_predict_seq=skeleton_predict_seq.squeeze(0)\n",
    "                skeleton_predict=torch.zeros(63).to(device)\n",
    "                for j in range(window_size):\n",
    "                    action[i+j,int(num[i+j])]=skeleton_predict_seq[j,:]\n",
    "                    num[i+j]+=1\n",
    "                    #print(f\"j={j},i+j={i+j},num[i+j}]={int(num[i+j])}\")\n",
    "                weights = compute_exponential_weights(int(num[i]), m).to(device)\n",
    "                for j in range(int(num[i])):\n",
    "                    skeleton_predict+=weights[j]*action[i,int(num[i])-1-j]\n",
    "                predictions[i]=skeleton_predict\n",
    "                for j in range(seq_length-1):\n",
    "                    skeleton_last[0,j]=skeleton_last[0,j+1]\n",
    "                skeleton_last[0,seq_length-1]=skeleton_predict\n",
    "        '''\n",
    "        # 予測の実行\n",
    "        print(\"Making predictions...\")\n",
    "        predictions=torch.zeros(min_length,63).to(device)\n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.FloatTensor(input_features).to(device)\n",
    "            input_tensor=input_tensor.to(device)\n",
    "            print(input_tensor.shape,skeleton_data.shape)\n",
    "            predictions=model(input_tensor,skeleton_data)\n",
    "        '''\n",
    "        print(f\"Prediction shape: {predictions.shape}\")\n",
    "        criterion = EnhancedSkeletonLoss(alpha=1.0,)\n",
    "        criterion1 = EnhancedSkeletonLoss_WithAngleConstrains(alpha=1.0,gamma=0.1,window_size=1)\n",
    "        predictionsa = predictions.unsqueeze(1)\n",
    "        skeleton_dataa = skeleton_data.unsqueeze(1)\n",
    "        loss=criterion(predictionsa,skeleton_dataa)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        loss1=criterion1(predictionsa,skeleton_dataa)\n",
    "        print(f\"Loss: {loss1}\")\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        return predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def save_predictions(predictions, output_file='./output/predicted_skeleton.csv'):\n",
    "    try:\n",
    "        # 予測結果をデータフレームに変換\n",
    "        num_joints = predictions.shape[1] // 3\n",
    "        columns = []\n",
    "        for i in range(num_joints):\n",
    "            columns.extend([f'X.{i*2+1}', f'Y.{i*2+1}', f'Z.{i*2+1}'])\n",
    "        \n",
    "        df_predictions = pd.DataFrame(predictions, columns=columns)\n",
    "        df_predictions.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting prediction process...\")\n",
    "        predictions = predict_skeleton()\n",
    "        \n",
    "        print(\"\\nSaving predictions...\")\n",
    "        save_predictions(predictions)\n",
    "        print(predictions)\n",
    "        \n",
    "        print(\"Prediction process completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b55a32-1294-4b56-9928-fe913d0ba81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
